<!DOCTYPE html>
<!--
    Forty by HTML5 UP
    html5up.net | @ajlkn
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
  -->
<html>

  <head>
	<title>Time no see you, let see what's going on | Cristian</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="/assets/css/main.css" />
	<!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->
	<link rel="apple-touch-icon" sizes="180x180" href="/assets/images/favicon_io/facapple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicon_io/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicon_io/favicon-16x16.png">
	<link rel="manifest" href="/assets/images/favicon_io/site.webmanifest">
</head>


  <body class="layout-post">

    <!-- Wrapper -->
<div id="wrapper">

<!-- Header -->
<header id="header">
	<a href="http://localhost:4000//" class="logo"><strong>Cristian</strong> <span>Quintero</span></a>
	<nav>
		<a href="#menu">Menu</a>
	</nav>
</header>

<!-- Menu -->
<nav id="menu">
	<ul class="links">
        
		    
		
		    
		
		    
		
		    
		        <li><a href="http://localhost:4000//">Home</a></li>
	    	
		
		    
		
		    
		
		    
		
		
		    
		
		    
		        <li><a href="http://localhost:4000/about_me.html">About Me</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/all_posts.html">All posts</a></li>
		    
		
		    
		
		    
		        <li><a href="http://localhost:4000/my_posts.html">My posts</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/whatilike.html">I like</a></li>
		    
		
	</ul>
	<ul class="actions vertical">
		<li><a href="#" class="button special fit">Get Started</a></li>
		<li><a href="#" class="button fit">Log In</a></li>
	</ul>
</nav>
 
    
    <!-- Enhanced Sidebar with Category-specific Posts -->
    
      
        
          
          
          
    
    <aside id="sidebar" class="sidebar">
      <div class="sidebar-content">
        
          <div class="sidebar-category-header">
            <h3>Quant Development</h3>
            <div class="category-indicator"></div>
          </div>
          <h4>Posts in this Category</h4>
          <ul class="recent-posts">
            
            
            
            
              <li class="no-related-posts">
                <p>No other posts in this category yet.</p>
              </li>
            
          </ul>
          
          <!-- Show link to all posts in this category -->
          <div class="sidebar-category-link">
            <a href="/my_posts.html" class="view-all-category">
              View All Quant Development Posts →
            </a>
          </div>
        
      </div>
    </aside>

    <!-- Sidebar toggle for mobile -->
    <button id="sidebar-toggle" class="sidebar-toggle" aria-label="Toggle sidebar">
      <i class="fa fa-bars"></i>
    </button>
    
    <!-- Sidebar overlay for mobile -->
    <div id="sidebar-overlay" class="sidebar-overlay"></div>
    
    <!-- Table of Contents Sidebar -->
    <button id="toc-toggle" class="toc-toggle" title="Show/Hide Table of Contents" aria-label="Show/Hide Table of Contents">
      <span aria-hidden="true">📑</span>
    </button>
    <nav id="toc-sidebar" class="toc-sidebar"></nav>

    <!-- Main -->
    <div id="main" class="alt">

      <!-- One -->
      <section id="one">
	<div class="inner">
		
			
        
          <header class="major">
            <h1>Time no see you, let see what's going on</h1>
          </header>
          
            <div class="post-image-container">
              <img src="http://localhost:4000/assets/images/04_llm_and_mcp/jarvis_.jpg" alt="Time no see you, let see what's going on" />
            </div>
          
          <p>Jun 29, 2025</p>
          <p><p>I am astonished by how technology evolves, creating new things with an extra layer of abstraction. A couple of years ago, when I was a professional software developer, I performed many tedious tasks that can now be carried out flawlessly with the help of a LLM assistant, such as <a href="https://github.com/features/copilot" target="_blank" rel="noopener">Github Copilot</a>, <a href="https://www.cursor.com/en" target="_blank" rel="noopener">Cursor</a>. There are many more than these two, including GPT 4.1 and Gemini. The evolution of these assistants enables anyone involved in coding to enhance their performance. This is not only because tedious tasks can be delegated to these assistants, who work more quickly, but also because they make better use of our most limited resource: time. I’m currently making use of one of them.</p>

<h1 id="whats-going-on">What’s going on?</h1>
<p>As someone interested in applying AI to finance, I’ve been tracking the evolution of LLM models that are then used for quantitative finance. The creativity of many quants and non-quants is simply impressive. As a quantitative analyst, I could not be more excited to test the scope of these applications. To this end, I have been preparing a set of experiments for a while to see how I can leverage my own models using LLM. However, many practitioners using these tools are aware that the information used to generate LLM outputs is not necessarily up to date. The programming language your models are written in is not that important; however, at the moment, I would say Python is the easiest language for testing purposes.</p>

<p>LLM models are a very wide topic, that I hope to talk about soon in another post, but just to have a glance I really recommend these books to get into the topic at some degree.</p>
<ul>
  <li><strong>Large Language Model-Based Solutions</strong> written by Shreyas Subramanian.</li>
  <li><strong>Hands-On Large Language Models</strong> written by Jay Alammar and Maarten Grootendorst.</li>
  <li><strong>Large Language Models Projects</strong> written by Pere Martra.</li>
  <li><strong>Prompt Engineering for LLMs</strong> written by John Berryman and Albert Ziegler.</li>
</ul>

<h1 id="ides-for-testing">IDEs for testing</h1>
<h2 id="github-copilot">Github Copilot</h2>
<p>For a while I’ve been using VS Code as IDE by preference for models development. It is exposed to a diversified number of extensions that leverage the development using different programming languages in the same IDE. Among other attributes the favorite one today for me is Github Copilot. This is an AI-powered coding assistant that may use of different LLM models such as GPT 4.1 or Gemini 2.5 for development. The usage is quite intuitive after a serie of iterations the IDE has received improving the user experience last year. Different modes are available as shown in <a href="https://learn.microsoft.com/en-us/shows/visual-studio-code/ask-edit-and-agent-in-depth-overview-of-github-copilot-chat-modes" target="_blank" rel="noopener">this video</a>. I think, the usage is almost self-explanatory once you watch the video, but I’d like to emphasize the Agent mode.</p>

<p>I’ve asked Github Copilot whether it uses RAG models, and output is self-explanatory, below.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GitHub Copilot does use some RAG-like techniques, but it's not a pure RAG system. Here's how it works:

🟢 **What GitHub Copilot uses:**
- Code context retrieval: Analyzes your current file, open tabs, and workspace to understand context
- Semantic search: Searches your codebase for relevant patterns and examples
- Real-time context: Incorporates your recent edits, cursor position, and file structure
- Repository-aware suggestions: Understands your project's coding patterns and dependencies

🟡 **However, it's not traditional RAG because:**
- It doesn't maintain a separate vector database of external documents
- It doesn't retrieve information from external knowledge bases in real-time
- The "retrieval" is more about code context rather than document retrieval

🟣 **The hybrid approach: GitHub Copilot combines:**
- Pre-trained knowledge from its training on public code repositories
- Context-aware retrieval from your current workspace
- LLM generation to produce relevant code suggestions

So while it uses retrieval-augmented techniques, it's more accurate to say it uses "context-aware generation" rather than pure RAG. The retrieval component is focused on your immediate coding context rather than external knowledge sources.
---

For true RAG in coding, you'd typically see systems that:
- Maintain vector databases of documentation
- Retrieve relevant docs/examples from external sources
- Combine retrieved information with LLM generation

**GitHub Copilot is more of a context-aware code completion tool that leverages both pre-trained knowledge and real-time code analysis.**
</code></pre></div></div>

<p><strong>Why Github Copilot and not Claude, for instance? Well, this excercise is performed in Linux, and Claude doesn’t have package to download and install in Linux.</strong></p>

<h1 id="models-context">Models Context</h1>
<h2 id="agentic-ai">Agentic AI</h2>
<p>Agentic AI has gained popularity and it refers to AI systems designed to act autonomously with long tasks that adaptively to plan and execute sub-tasks without constant human input. Let’s put it in simple words, it likes divide and conquer. Unlike traditional AI models that respond to prompts, agentic AI operates more like software agents that can take initiative, make decisions for planing the string of sub-tasks, and revise the strategies based on feedback. These systems combine LLMs with tools, and this is my favorite part, that can be customized to not depending of last updated vector store databases (see <strong>RAG-driven generative AI</strong> by Denis Rothman, <strong>Generative AI with LangChain</strong> by Ben Auffarth, and/or <strong>AI Engineering</strong> by Chip Huyen).</p>

<p>The key difference between agentic AI and generic/standard AI lies in autonomy and goal-directed behavior. Whereas regular AI (like ChatGPT) excels at solving single problems or providing task-specific outputs, it lacks persistence, self-direction, and memory. Conversely, Agentic AI behaves more like a digital assistant by setting subgoals, recall past context, learn from outcomes, and even interact with humans over time. Consequently, agentic AI systems represent a major shift from AI tools that wait for instructions to systems that actively work alongside or on behalf of humans in dynamic, real world tasks.</p>

<h2 id="why-rag-is-important">Why RAG is important?</h2>
<p>Well, LLM models themselve are exposed to drawbacks such as hallucination, I mentioned before the limited knowledge given the data during training phase, and the most relevant for quants, IMO, lack of access to private/dynamic information. In spite this shortcomings, LLM models have been complemented by RAG (Retreival-Augmented Generation) models. As Denis Rothman describes the process in <strong>RAG-driven generative AI</strong> book, chapter 1, RAG model is compraised by two stages aimed to improve accuracy of LLM responses. The first stage is <em>retrieval</em> that consists on collecting relevant resources associated with the topic in context. Afterwards, the stage coined <em>generation</em> depicts the action of using relevant sources to produce the output via LLM models. An interesting coding example of how to parse PDF docs for RAG <a href="https://cookbook.openai.com/examples/parse_pdf_docs_for_rag" target="_blank" rel="noopener">here</a>.</p>

<h2 id="why-is-then-agent-mode-in-gh-copilot-interesting-for-quants">Why is then Agent Mode in GH Copilot interesting for Quants?</h2>
<p>My intuition says me the answer to this question may be controversial. Nonetheless, let me explore from my personal point of view. One of our tasks is being able for quantitatively to validate/model/develop tools that can serve for different processes in finance, such as portfolio management, algorithmic trading, risk management, so on and so forth. Across this process, we manage to optimize our resources such that we can dedicate our time to the main goal, that is to produce models that yield an added value for company our portfolios. Just in case, I’m not refering to regulatory aspects, but definitely given the standarized structure many cases, it could be included. Thereby, tools that reduce the operative aspect to do so gain terrain in our toolkit. So, why could LLM models be in our toolkit?. IMO, to reduce our human effor doing manual stuff creating the tool to do so. I’ve not said anything new, since this maight be quite related to DevOps in software engineering.</p>

<h2 id="mcp-servers-and-their-relevance-for-agentic-ai-and-rag">MCP Servers and Their Relevance for Agentic AI and RAG</h2>

<p><strong>MCP (Model Context Protocol) servers</strong> are specialized endpoints that enable large language models (LLMs) and agentic AI systems to interact with external tools, data sources, or computational resources in a standardized way. In the context of agentic AI and RAG, MCP servers provide a protocol for LLMs to send structured requests (such as data queries, analytics tasks, or model inference) and receive results in a predictable format.</p>

<p>This protocol-driven approach allows agentic systems to dynamically extend their capabilities by leveraging external services for tasks like financial calculations, data retrieval, or portfolio analytics—without embedding all logic within the LLM itself. For RAG workflows, MCP servers can act as retrieval endpoints, supplying up-to-date or proprietary information that enhances the relevance and accuracy of generated responses. In quantitative finance, using MCP servers ensures secure, auditable, and modular integration of advanced analytics into agentic or LLM-powered solutions.</p>

<h1 id="lets-get-to-work">Let’s get to Work!</h1>
<p>Now that you have the post’s context, let’s get to work!. The goal is to create a MCP server that allows to calculate <em>Historical Value at Risk (HVaR)</em> based on a CSV file with specific structure containing the historical prices. For this example we’ll take as input data the file found <a href="https://github.com/craquinterogo/quantitative_finance/tree/master/LLM/mcp-server/datasource" target="_blank" rel="noopener">here</a>. It contains daily historical prices of RGTI (<a href="https://www.rigetti.com/" target="_blank" rel="noopener">Rigetti</a>) stock.</p>

<h2 id="setup">Setup</h2>
<p>The set of configurations are pretty simple. For the sake of this example, I’ve created a very small library that I can reuse in both MCP server and jupyter notebook to check the answer. Library may be found <a href="https://github.com/craquinterogo/quantitative_finance/tree/master/LLM/mcp-server" target="_blank" rel="noopener">here</a>. It is stored in <code class="language-plaintext highlighter-rouge">my_var</code> folder, wheras server correspond to <code class="language-plaintext highlighter-rouge">server.py</code> file.</p>

<h3 id="my-var-library">my-var library</h3>

<h4 id="var-module">Var Module</h4>
<p>This module is responsible for HVaR and HCVaR calculations, together with return calculations. Thereby, <code class="language-plaintext highlighter-rouge">var.py</code> file contains two functions as shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="k">def</span> <span class="nf">calculate_returns</span><span class="p">(</span><span class="n">prices</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]):</span>
    <span class="sh">"""</span><span class="s">
    Calculate simple returns from a sequence of prices.
    </span><span class="sh">"""</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">We are in calculate_returns function, receiving </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">prices</span><span class="p">)</span><span class="si">}</span><span class="s"> data points.</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">prices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">prices</span><span class="p">).</span><span class="nf">flatten</span><span class="p">()</span>
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">prices</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">At least two prices are required to calculate returns.</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="p">(</span><span class="n">prices</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">prices</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="n">prices</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">returns</span><span class="p">.</span><span class="nf">tolist</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">calculate_var_cvar_historical</span><span class="p">(</span><span class="n">returns</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">significance_level</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Calculate historical VaR and CVaR for a given returns series or array.
    </span><span class="sh">"""</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">significance_level</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="n">significance_level</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="n">significance_level</span><span class="p">)</span>
        <span class="c1"># print("Step 1: Converting returns to numpy array and flattening.")
</span>        <span class="n">returns</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">returns</span><span class="p">).</span><span class="nf">flatten</span><span class="p">()</span>
        <span class="c1"># print(returns)
</span>        <span class="c1"># print(f"Step 2: Filtering NaNs. Input length: {len(returns)}")
</span>        <span class="n">returns</span> <span class="o">=</span> <span class="n">returns</span><span class="p">[</span><span class="o">~</span><span class="n">np</span><span class="p">.</span><span class="nf">isnan</span><span class="p">(</span><span class="n">returns</span><span class="p">)]</span>
        <span class="c1"># print(f"Step 3: After filtering NaNs. Length: {len(returns)}")
</span>        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Returns array is empty.</span><span class="sh">"</span><span class="p">)</span>

        <span class="c1"># print("Step 4: Sorting returns ascending.")
</span>        <span class="n">sorted_returns</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span>
        <span class="n">n</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">sorted_returns</span><span class="p">)</span>
        <span class="c1"># print(f"Step 5: Number of sorted returns: {n}")
</span>        <span class="n">var_index</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">floor</span><span class="p">(</span><span class="n">significance_level</span> <span class="o">*</span> <span class="n">n</span><span class="p">))</span>
        <span class="n">var_index</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">min</span><span class="p">(</span><span class="n">var_index</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="c1"># print(f"Step 6: Calculated var_index: {var_index}")
</span>        <span class="n">var_historical</span> <span class="o">=</span> <span class="n">sorted_returns</span><span class="p">[</span><span class="n">var_index</span><span class="p">]</span>
        <span class="c1"># print(f"Step 7: VaR (historical): {var_historical}")
</span>
        <span class="n">cvar_historical</span> <span class="o">=</span> <span class="n">sorted_returns</span><span class="p">[:</span><span class="n">var_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">].</span><span class="nf">mean</span><span class="p">()</span>
        <span class="c1"># print(f"Step 8: CVaR (historical): {cvar_historical}")
</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">significance_level</span><span class="sh">'</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">significance_level</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">n_points</span><span class="sh">'</span><span class="p">:</span> <span class="n">n</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">var_historical</span><span class="sh">'</span><span class="p">:</span> <span class="n">var_historical</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">cvar_historical</span><span class="sh">'</span><span class="p">:</span> <span class="n">cvar_historical</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="c1"># print("Step 9: Returning results.")
</span>        <span class="k">return</span> <span class="n">results</span>
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Exception occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">raise</span>
</code></pre></div></div>

<h4 id="utils-module">Utils Module</h4>

<p>On the other hand, the module <code class="language-plaintext highlighter-rouge">utils.py</code> contains one function focused on reading the content in CSV file, as follows.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="k">def</span> <span class="nf">extract_column_from_csv</span><span class="p">(</span><span class="n">csv_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">column</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">days</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">round</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    Extract the last N values from a specified column in a CSV file containing historical prices.

    Parameters:
    csv_path: str, absolute path to the CSV file
    column: str, column name to extract values from
    days: int, number of last days to extract

    Returns:
    list: last N values from the specified column
    </span><span class="sh">"""</span>
    <span class="c1"># Try to automatically detect decimal separator
</span>    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">csv_path</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">,</span> <span class="n">decimal</span><span class="o">=</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">column</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Column </span><span class="sh">'</span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="sh">'</span><span class="s"> not found in CSV file.</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="nf">dropna</span><span class="p">().</span><span class="nf">round</span><span class="p">(</span><span class="nb">round</span><span class="p">).</span><span class="n">values</span>
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">values</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">days</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">CSV contains only </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">values</span><span class="p">)</span><span class="si">}</span><span class="s"> rows, but </span><span class="si">{</span><span class="n">days</span><span class="si">}</span><span class="s"> requested.</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">values</span><span class="p">[</span><span class="o">-</span><span class="n">days</span><span class="p">:].</span><span class="nf">tolist</span><span class="p">()</span>
</code></pre></div></div>

<p>These two modules will allow the server to interact with <code class="language-plaintext highlighter-rouge">GPT 4.1</code> which is the LLM model selected for this test since it has shown a good performance in coding tasks. I did compare the performance respect to <code class="language-plaintext highlighter-rouge">Claude Sonnet 4</code> in Agent mode, and the main drawback observed is the speed to execute the tasks as well as verbosity, where former displays more verbose than <code class="language-plaintext highlighter-rouge">GPT 4.1</code>.</p>

<h2 id="mcp-server">MCP Server</h2>

<p>MCP server is built using <code class="language-plaintext highlighter-rouge">mcp</code> library by means <a href="https://modelcontextprotocol.io/introduction" target="_blank" rel="noopener">Model Context Protocol</a> - <a href="https://github.com/modelcontextprotocol/python-sdk" target="_blank" rel="noopener">Python SDK</a>. Code below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># server.py
</span><span class="kn">from</span> <span class="n">mcp.server.fastmcp</span> <span class="kn">import</span> <span class="n">FastMCP</span>
<span class="kn">import</span> <span class="n">math</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">import</span> <span class="n">yfinance</span> <span class="k">as</span> <span class="n">yf</span>
<span class="kn">from</span> <span class="n">my_var.var</span> <span class="kn">import</span> <span class="n">calculate_returns</span><span class="p">,</span> <span class="n">calculate_var_cvar_historical</span>
<span class="kn">from</span> <span class="n">my_var.utils</span> <span class="kn">import</span> <span class="n">extract_column_from_csv</span>

<span class="kn">import</span> <span class="n">warnings</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">warnings</span><span class="p">.</span><span class="nf">filterwarnings</span><span class="p">(</span><span class="sh">'</span><span class="s">ignore</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Create an MCP server
</span><span class="n">mcp</span> <span class="o">=</span> <span class="nc">FastMCP</span><span class="p">(</span><span class="sh">"</span><span class="s">Quant-MCP-server</span><span class="sh">"</span><span class="p">)</span>

<span class="nd">@mcp.tool</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">extract_column_from_csv_mcp</span><span class="p">(</span><span class="n">csv_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">column</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">days</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">round</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    Extract the last N values from a specified column in a CSV file containing historical prices.

    Parameters:
    csv_path: str, absolute path to the CSV file
    column: str, column name to extract values from
    days: int, number of last days to extract

    Returns:
    list: last N values from the specified column

    input example:
    {
        </span><span class="sh">"</span><span class="s">csv_path</span><span class="sh">"</span><span class="s">: </span><span class="sh">"</span><span class="s">/path/to/prices.csv</span><span class="sh">"</span><span class="s">,
        </span><span class="sh">"</span><span class="s">column</span><span class="sh">"</span><span class="s">: </span><span class="sh">"</span><span class="s">Close</span><span class="sh">"</span><span class="s">,
        </span><span class="sh">"</span><span class="s">days</span><span class="sh">"</span><span class="s">: 30,
        </span><span class="sh">"</span><span class="s">round</span><span class="sh">"</span><span class="s">: 4
    }

    output example:
    [3, 1.02, 1.05, 1.05, 1.02, 1.03, 1.05, 1.02, 1.02, 0.9864, 1.04, 0.98, 0.88, 0.9531, 0.9644, 0.8967, 0.9061, 1.05, 1.07, 0.9987, 0.9491, 0.96, 0.992, 1.04, 1.04, 1.08, 1.02, 1.08, 1.19, 1.28, 1.17, 1.09, 1.04, 1.05, 1.12, 1.02, 1.04, 1.07, 1.03, 0.9904, 1.03, 0.948, 0.8972, 0.7961, 0.8462, 0.7778, 0.8305, 0.7902, 0.8537, 0.8966, 0.8409, 0.8856, 0.8982, 0.87, 0.836, 0.9275, 0.9068, 0.9659, 0.9459, 0.91, 0.8599, 0.8807, 0.9164, 0.8312, 0.835, 0.8, 0.7536, 0.6881, 0.7336, 0.8044, 0.8202, 0.8526, 0.8216, 0.8404, 0.82, 0.8151, 0.7939, 0.765, 0.76, 0.7402, 0.7781, 0.8102, 0.7831, 0.753, 0.7644, 0.748, 0.7849, 0.7847, 0.7789, 0.7659, 0.7555, 0.8164, 0.8422, 0.8105, 0.92, 0.951, 1.11, 1.28, 1.23, 1.21, 1.2, 1.22, 1.5, 1.46, 1.41, 1.32, 1.23, 1.13, 1.12, 1.2, 1.43, 1.51, 1.52, 1.49, 1.7, 1.55, 1.41, 1.3, 1.31, 1.35, 1.48, 1.74, 2.75, 2.2, 2.4, 3.05, 3.02, 3.14, 3.11, 3.18, 4.38, 4.47, 6.49, 7.38, 5.97, 7.16, 8.43, 11.13, 10.69, 7.47, 9.37, 10.96, 11.35, 15.44, 17.08, 17.0, 15.26, 20.0, 19.02, 19.51, 18.39, 10.04, 8.93, 6.05, 8.95, 10.94, 11.24, 9.83, 13.98, 13.91, 13.47, 13.2, 12.45, 13.08, 12.66, 12.3, 13.17, 13.47, 13.72, 13.83, 13.29, 12.85, 12.35, 11.02, 11.75, 12.25, 11.81, 10.52, 11.03, 11.47, 10.75, 10.12, 9.03, 9.28, 8.03, 8.46, 7.7, 7.86, 8.18, 8.51, 9.35, 7.91, 8.05, 8.95, 8.75, 11.22, 11.16, 10.26, 9.905, 8.99, 9.07, 9.78, 9.82, 9.18, 8.47, 8.15, 7.92, 7.81, 8.49, 8.15, 7.5, 8.33, 7.72, 9.39, 9.42, 9.1, 8.86, 8.62, 8.25, 8.32, 8.11, 8.57, 9.11, 9.3, 9.37, 9.22, 8.86, 8.87, 9.14, 10.63, 9.695, 9.785, 9.25, 10.31, 10.58, 11.55, 9.865, 11.64, 11.54, 11.85, 12.05, 11.92, 10.96, 13.86, 14.02, 14.19, 14.16, 13.15, 12.11, 12.26, 12.04, 11.82, 11.3186]
    </span><span class="sh">"""</span>
    <span class="c1"># Try to automatically detect decimal separator
</span>    <span class="n">list_prices</span> <span class="o">=</span> <span class="nf">extract_column_from_csv</span><span class="p">(</span><span class="n">csv_path</span><span class="o">=</span><span class="n">csv_path</span><span class="p">,</span> <span class="n">column</span><span class="o">=</span><span class="n">column</span><span class="p">,</span><span class="n">days</span><span class="o">=</span><span class="n">days</span><span class="p">,</span> <span class="nb">round</span><span class="o">=</span><span class="nb">round</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">list_prices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">list_prices</span>

<span class="nd">@mcp.tool</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">calculate_returns_mcp</span><span class="p">(</span><span class="n">prices</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]):</span>
    <span class="sh">"""</span><span class="s">
    Calculate simple returns from a sequence of p
    return calculate_var_cvar_historical(returns=returns, significance_level=significance_level)rices.

    Parameters:
    prices: array-like, sequence of prices

    Returns:
    list: returns as a list of floats

    input example:
    {
    </span><span class="sh">"</span><span class="s">prices</span><span class="sh">"</span><span class="s">: [21.14,21.15,21.15,21.16,21.18,21.17,21.18,21.16,21.09,21.13,21.13,21.15,21.13,21.02,21.02,21.01,21.03,21.04,21.03,21.04,21.05,21.08,21.07,21.1,21.11,21.14,21.15,21.15,21.16,21.16,21.16,21.17,21.04,21.02,21.0,20.96,20.98,20.99,20.97,21.0,20.98,20.89,20.69,20.87,20.87,20.97,20.96,20.97,20.98,20.98,21.03,21.05,20.89,20.89,20.91,20.89,20.93,20.94,20.95,21.01,21.04,21.07,21.02,21.01,21.03]
    }
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="nf">calculate_returns</span><span class="p">(</span><span class="n">prices</span><span class="o">=</span><span class="n">prices</span><span class="p">)</span>

<span class="c1"># Add an addition tool
</span><span class="nd">@mcp.tool</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">calculate_var_cvar_historical_mcp</span><span class="p">(</span><span class="n">returns</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">significance_level</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Calculate historical VaR and CVaR for a given returns series or array.

    Parameters:
    returns: array-like, series of returns
    significance_level: float, confidence level (0.05 for 95% VaR/CVaR)

    Returns:
    dict: VaR and CVaR calculations

    input example:
    { 
        </span><span class="sh">"</span><span class="s">returns</span><span class="sh">"</span><span class="s">: [1.14,2.1,2.5,2.1,..], 
        </span><span class="sh">"</span><span class="s">significance_level</span><span class="sh">"</span><span class="s">: 0.05
    }
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="nf">calculate_var_cvar_historical</span><span class="p">(</span><span class="n">returns</span><span class="o">=</span><span class="n">returns</span><span class="p">,</span> <span class="n">significance_level</span><span class="o">=</span><span class="n">significance_level</span><span class="p">)</span>


<span class="nd">@mcp.resource</span><span class="p">(</span><span class="sh">"</span><span class="s">yfinance-data://{ticker}/{period}</span><span class="sh">"</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">download_yfinance_data</span><span class="p">(</span><span class="n">ticker</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">period</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">'</span><span class="s">2y</span><span class="sh">'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Download historical data from Yahoo Finance for a given ticker and period.

    Parameters:
    ticker: str, stock ticker symbol
    period: str, data period (</span><span class="sh">'</span><span class="s">1y</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">2y</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">5y</span><span class="sh">'</span><span class="s">, etc.)

    Returns:
    dict: Historical price data as a dictionary
    </span><span class="sh">"""</span>
    <span class="n">stock</span> <span class="o">=</span> <span class="n">yf</span><span class="p">.</span><span class="nc">Ticker</span><span class="p">(</span><span class="n">ticker</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">stock</span><span class="p">.</span><span class="nf">history</span><span class="p">(</span><span class="n">period</span><span class="o">=</span><span class="n">period</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">data</span><span class="p">.</span><span class="n">empty</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">No data found for ticker </span><span class="si">{</span><span class="n">ticker</span><span class="si">}</span><span class="s"> and period </span><span class="si">{</span><span class="n">period</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">().</span><span class="nf">to_dict</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="sh">'</span><span class="s">list</span><span class="sh">'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">Main function to run the MCP server</span><span class="sh">"""</span>
    <span class="n">mcp</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">transport</span><span class="o">=</span><span class="sh">'</span><span class="s">sse</span><span class="sh">'</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="nf">main</span><span class="p">()</span>
</code></pre></div></div>

<p>Likely you’ll be impressed, as I was, when you realize that for running it you only need to execute the common python command as follows.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">&gt;</span><span class="w"> </span>python server.py
</code></pre></div></div>

<h2 id="github-copilot---agent-mode">Github Copilot - Agent Mode</h2>

<p>Adding the server in Agent mode, is also straightforward. Instructions can be found in <a href="https://code.visualstudio.com/docs/copilot/chat/mcp-servers">VS Code webpage</a>. Nonetheless, I want to make only two comments respect official documentation.</p>

<ul>
  <li>As observed in <code class="language-plaintext highlighter-rouge">main()</code> function of <code class="language-plaintext highlighter-rouge">server.py</code> file, I’m passing the parameter <strong>transport=”sse”</strong>. This is one of the three modes that MCP server has to communicate with MCP client (If there is a server there is always a client). So, The setup in <code class="language-plaintext highlighter-rouge">mcp.json</code> file, that is created when selected the option <strong>Workspace Setings</strong> when adding a server ought to look like</li>
</ul>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"servers"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"quant-mcp-server"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
            </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sse"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"url"</span><span class="p">:</span><span class="w"> </span><span class="s2">"http://127.0.0.1:8000/sse"</span><span class="w">
        </span><span class="p">}</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p><strong>The point is that the endpoint of the added server must be <code class="language-plaintext highlighter-rouge">/sse</code>, otherwise you cannot connect to MCP server from Agent mode with this setup.</strong></p>

<ul>
  <li>When testing the server, and you disconnect it when Agent mode is using the server, it is useful to switch to <code class="language-plaintext highlighter-rouge">edit</code> or <code class="language-plaintext highlighter-rouge">ask</code> mode to let Github Copilot attempt to refresh the description of tools in MCP server.</li>
</ul>

<h2 id="execution">Execution</h2>
<p>As I said the goal is to calculate HVaR and SVaR given the input from CSV file. On this regard, the designed prompt I used to test the server via Agent mode in Github Copilot is shown below.</p>

<blockquote>
  <p>identify the absolute path of CSV in this workspace and then use quant-mcp-server to retrieve the last 253 days of column “close”, then used to calculate the returns and subsequently letting to calculate VaR and CVaR. Take into account the expected input format. No any additional script is required. Be flexible with the number of decimals such that you don’t modify outputs. Not apply any rouding or truncate.</p>
</blockquote>

<p>Testing codes in Github <a href="https://github.com/craquinterogo/quantitative_finance/tree/master/LLM/mcp-server" target="_blank" rel="noopener">here</a>. Stages are described as follow.</p>

<h3 id="reading-csv-file---first-step">Reading CSV file - first step</h3>
<p>As displayed in <code class="language-plaintext highlighter-rouge">utils.py</code> file above, there is a function called <code class="language-plaintext highlighter-rouge">extract_column_from_csv</code> that receives the path to the csv file, the column name and the number of observations to be extracted. It assumes the file content depicts historical prices sorted ascending. By calling the tool <code class="language-plaintext highlighter-rouge">extract_column_from_csv_mcp</code> in <code class="language-plaintext highlighter-rouge">server.py</code>, the fucntion in <code class="language-plaintext highlighter-rouge">utils.py</code> is called. As result, the array with prices is collected by <code class="language-plaintext highlighter-rouge">GPT 4.1</code> then passed by Github Copilot agent to next task designed by it to accomplish the goal. Image below shows the output of this first step. To let the agent continue with next task the user must press the button <strong>Continue</strong>.</p>

<p><img src="http://localhost:4000/assets/images/04_llm_and_mcp/01_00_calling_mcp_server.png" alt="Calling MCP Server" /></p>

<p>Input below:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"column"</span><span class="p">:</span><span class="w"> </span><span class="s2">"close"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"csv_path"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/..&lt;your absolute path to csv file&gt;../data_source/historical_prices/1_day_5_Y/20250605/RGTI_193552.csv"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"days"</span><span class="p">:</span><span class="w"> </span><span class="mi">253</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<h3 id="calculating-returns---second-step">Calculating Returns - Second step</h3>
<p>As part of the common flow to calculate Historical VaR, the returns need to be calculated. Subsequently, GH Copilot agent calls MCP tool <code class="language-plaintext highlighter-rouge">calculate_returns_mcp</code>. This tool internally calls the function <code class="language-plaintext highlighter-rouge">my_var.var.calculate_var_cvar_historical</code> receiving as parameter the list of prices, obtained from call MCP tool <code class="language-plaintext highlighter-rouge">extract_column_from_csv_mcp</code>. Image below shows the input to be sent to the second task, passing as parameter the json with prices. Click to button <strong>Continue</strong> is expected to proceed.</p>

<p><img src="http://localhost:4000/assets/images/04_llm_and_mcp/01_01_calling_mcp_server.png" alt="Calling MCP Server" /></p>

<p>Input below:
s</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"prices"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="mf">1.03</span><span class="p">,</span><span class="w"> </span><span class="mf">1.02</span><span class="p">,</span><span class="w"> </span><span class="mf">1.05</span><span class="p">,</span><span class="w"> </span><span class="mf">1.05</span><span class="p">,</span><span class="w"> </span><span class="mf">1.02</span><span class="p">,</span><span class="w"> </span><span class="mf">1.03</span><span class="p">,</span><span class="w"> </span><span class="mf">1.05</span><span class="p">,</span><span class="w"> </span><span class="mf">1.02</span><span class="p">,</span><span class="w"> </span><span class="mf">1.02</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9864</span><span class="p">,</span><span class="w"> </span><span class="mf">1.04</span><span class="p">,</span><span class="w"> </span><span class="mf">0.98</span><span class="p">,</span><span class="w"> </span><span class="mf">0.88</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9531</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9644</span><span class="p">,</span><span class="w">
    </span><span class="mf">0.8967</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9061</span><span class="p">,</span><span class="w"> </span><span class="mf">1.05</span><span class="p">,</span><span class="w"> </span><span class="mf">1.07</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9987</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9491</span><span class="p">,</span><span class="w"> </span><span class="mf">0.96</span><span class="p">,</span><span class="w"> </span><span class="mf">0.992</span><span class="p">,</span><span class="w"> </span><span class="mf">1.04</span><span class="p">,</span><span class="w"> </span><span class="mf">1.04</span><span class="p">,</span><span class="w"> </span><span class="mf">1.08</span><span class="p">,</span><span class="w"> </span><span class="mf">1.02</span><span class="p">,</span><span class="w"> </span><span class="mf">1.08</span><span class="p">,</span><span class="w"> </span><span class="mf">1.19</span><span class="p">,</span><span class="w"> </span><span class="mf">1.28</span><span class="p">,</span><span class="w">
    </span><span class="mf">1.17</span><span class="p">,</span><span class="w"> </span><span class="mf">1.09</span><span class="p">,</span><span class="w"> </span><span class="mf">1.04</span><span class="p">,</span><span class="w"> </span><span class="mf">1.05</span><span class="p">,</span><span class="w"> </span><span class="mf">1.12</span><span class="p">,</span><span class="w"> </span><span class="mf">1.02</span><span class="p">,</span><span class="w"> </span><span class="mf">1.04</span><span class="p">,</span><span class="w"> </span><span class="mf">1.07</span><span class="p">,</span><span class="w"> </span><span class="mf">1.03</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9904</span><span class="p">,</span><span class="w"> </span><span class="mf">1.03</span><span class="p">,</span><span class="w"> </span><span class="mf">0.948</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8972</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7961</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8462</span><span class="p">,</span><span class="w">
    </span><span class="mf">0.7778</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8305</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7902</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8537</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8966</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8409</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8856</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8982</span><span class="p">,</span><span class="w"> </span><span class="mf">0.87</span><span class="p">,</span><span class="w"> </span><span class="mf">0.836</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9275</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9068</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9659</span><span class="p">,</span><span class="w">
    </span><span class="mf">0.9459</span><span class="p">,</span><span class="w"> </span><span class="mf">0.91</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8599</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8807</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9164</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8312</span><span class="p">,</span><span class="w"> </span><span class="mf">0.835</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7536</span><span class="p">,</span><span class="w"> </span><span class="mf">0.6881</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7336</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8044</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8202</span><span class="p">,</span><span class="w">
    </span><span class="mf">0.8526</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8216</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8404</span><span class="p">,</span><span class="w"> </span><span class="mf">0.82</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8151</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7939</span><span class="p">,</span><span class="w"> </span><span class="mf">0.765</span><span class="p">,</span><span class="w"> </span><span class="mf">0.76</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7402</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7781</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8102</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7831</span><span class="p">,</span><span class="w"> </span><span class="mf">0.753</span><span class="p">,</span><span class="w">
    </span><span class="mf">0.7644</span><span class="p">,</span><span class="w"> </span><span class="mf">0.748</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7849</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7847</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7789</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7659</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7555</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8164</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8422</span><span class="p">,</span><span class="w"> </span><span class="mf">0.8105</span><span class="p">,</span><span class="w"> </span><span class="mf">0.92</span><span class="p">,</span><span class="w"> </span><span class="mf">0.951</span><span class="p">,</span><span class="w"> </span><span class="mf">1.11</span><span class="p">,</span><span class="w">
    </span><span class="mf">1.28</span><span class="p">,</span><span class="w"> </span><span class="mf">1.23</span><span class="p">,</span><span class="w"> </span><span class="mf">1.21</span><span class="p">,</span><span class="w"> </span><span class="mf">1.2</span><span class="p">,</span><span class="w"> </span><span class="mf">1.22</span><span class="p">,</span><span class="w"> </span><span class="mf">1.5</span><span class="p">,</span><span class="w"> </span><span class="mf">1.46</span><span class="p">,</span><span class="w"> </span><span class="mf">1.41</span><span class="p">,</span><span class="w"> </span><span class="mf">1.32</span><span class="p">,</span><span class="w"> </span><span class="mf">1.23</span><span class="p">,</span><span class="w"> </span><span class="mf">1.13</span><span class="p">,</span><span class="w"> </span><span class="mf">1.12</span><span class="p">,</span><span class="w"> </span><span class="mf">1.2</span><span class="p">,</span><span class="w"> </span><span class="mf">1.43</span><span class="p">,</span><span class="w"> </span><span class="mf">1.51</span><span class="p">,</span><span class="w"> </span><span class="mf">1.52</span><span class="p">,</span><span class="w"> </span><span class="mf">1.49</span><span class="p">,</span><span class="w">
    </span><span class="mf">1.7</span><span class="p">,</span><span class="w"> </span><span class="mf">1.55</span><span class="p">,</span><span class="w"> </span><span class="mf">1.41</span><span class="p">,</span><span class="w"> </span><span class="mf">1.3</span><span class="p">,</span><span class="w"> </span><span class="mf">1.31</span><span class="p">,</span><span class="w"> </span><span class="mf">1.35</span><span class="p">,</span><span class="w"> </span><span class="mf">1.48</span><span class="p">,</span><span class="w"> </span><span class="mf">1.74</span><span class="p">,</span><span class="w"> </span><span class="mf">2.75</span><span class="p">,</span><span class="w"> </span><span class="mf">2.2</span><span class="p">,</span><span class="w"> </span><span class="mf">2.4</span><span class="p">,</span><span class="w"> </span><span class="mf">3.05</span><span class="p">,</span><span class="w"> </span><span class="mf">3.02</span><span class="p">,</span><span class="w"> </span><span class="mf">3.14</span><span class="p">,</span><span class="w"> </span><span class="mf">3.11</span><span class="p">,</span><span class="w"> </span><span class="mf">3.18</span><span class="p">,</span><span class="w"> </span><span class="mf">4.38</span><span class="p">,</span><span class="w">
    </span><span class="mf">4.47</span><span class="p">,</span><span class="w"> </span><span class="mf">6.49</span><span class="p">,</span><span class="w"> </span><span class="mf">7.38</span><span class="p">,</span><span class="w"> </span><span class="mf">5.97</span><span class="p">,</span><span class="w"> </span><span class="mf">7.16</span><span class="p">,</span><span class="w"> </span><span class="mf">8.43</span><span class="p">,</span><span class="w"> </span><span class="mf">11.13</span><span class="p">,</span><span class="w"> </span><span class="mf">10.69</span><span class="p">,</span><span class="w"> </span><span class="mf">7.47</span><span class="p">,</span><span class="w"> </span><span class="mf">9.37</span><span class="p">,</span><span class="w"> </span><span class="mf">10.96</span><span class="p">,</span><span class="w"> </span><span class="mf">11.35</span><span class="p">,</span><span class="w"> </span><span class="mf">15.44</span><span class="p">,</span><span class="w"> </span><span class="mf">17.08</span><span class="p">,</span><span class="w"> </span><span class="mi">17</span><span class="p">,</span><span class="w"> </span><span class="mf">15.26</span><span class="p">,</span><span class="w">
    </span><span class="mi">20</span><span class="p">,</span><span class="w"> </span><span class="mf">19.02</span><span class="p">,</span><span class="w"> </span><span class="mf">19.51</span><span class="p">,</span><span class="w"> </span><span class="mf">18.39</span><span class="p">,</span><span class="w"> </span><span class="mf">10.04</span><span class="p">,</span><span class="w"> </span><span class="mf">8.93</span><span class="p">,</span><span class="w"> </span><span class="mf">6.05</span><span class="p">,</span><span class="w"> </span><span class="mf">8.95</span><span class="p">,</span><span class="w"> </span><span class="mf">10.94</span><span class="p">,</span><span class="w"> </span><span class="mf">11.24</span><span class="p">,</span><span class="w"> </span><span class="mf">9.83</span><span class="p">,</span><span class="w"> </span><span class="mf">13.98</span><span class="p">,</span><span class="w"> </span><span class="mf">13.91</span><span class="p">,</span><span class="w"> </span><span class="mf">13.47</span><span class="p">,</span><span class="w"> </span><span class="mf">13.2</span><span class="p">,</span><span class="w">
    </span><span class="mf">12.45</span><span class="p">,</span><span class="w"> </span><span class="mf">13.08</span><span class="p">,</span><span class="w"> </span><span class="mf">12.66</span><span class="p">,</span><span class="w"> </span><span class="mf">12.3</span><span class="p">,</span><span class="w"> </span><span class="mf">13.17</span><span class="p">,</span><span class="w"> </span><span class="mf">13.47</span><span class="p">,</span><span class="w"> </span><span class="mf">13.72</span><span class="p">,</span><span class="w"> </span><span class="mf">13.83</span><span class="p">,</span><span class="w"> </span><span class="mf">13.29</span><span class="p">,</span><span class="w"> </span><span class="mf">12.85</span><span class="p">,</span><span class="w"> </span><span class="mf">12.35</span><span class="p">,</span><span class="w"> </span><span class="mf">11.02</span><span class="p">,</span><span class="w"> </span><span class="mf">11.75</span><span class="p">,</span><span class="w"> </span><span class="mf">12.25</span><span class="p">,</span><span class="w">
    </span><span class="mf">11.81</span><span class="p">,</span><span class="w"> </span><span class="mf">10.52</span><span class="p">,</span><span class="w"> </span><span class="mf">11.03</span><span class="p">,</span><span class="w"> </span><span class="mf">11.47</span><span class="p">,</span><span class="w"> </span><span class="mf">10.75</span><span class="p">,</span><span class="w"> </span><span class="mf">10.12</span><span class="p">,</span><span class="w"> </span><span class="mf">9.03</span><span class="p">,</span><span class="w"> </span><span class="mf">9.28</span><span class="p">,</span><span class="w"> </span><span class="mf">8.03</span><span class="p">,</span><span class="w"> </span><span class="mf">8.46</span><span class="p">,</span><span class="w"> </span><span class="mf">7.7</span><span class="p">,</span><span class="w"> </span><span class="mf">7.86</span><span class="p">,</span><span class="w"> </span><span class="mf">8.18</span><span class="p">,</span><span class="w"> </span><span class="mf">8.51</span><span class="p">,</span><span class="w"> </span><span class="mf">9.35</span><span class="p">,</span><span class="w"> </span><span class="mf">7.91</span><span class="p">,</span><span class="w">
    </span><span class="mf">8.05</span><span class="p">,</span><span class="w"> </span><span class="mf">8.95</span><span class="p">,</span><span class="w"> </span><span class="mf">8.75</span><span class="p">,</span><span class="w"> </span><span class="mf">11.22</span><span class="p">,</span><span class="w"> </span><span class="mf">11.16</span><span class="p">,</span><span class="w"> </span><span class="mf">10.26</span><span class="p">,</span><span class="w"> </span><span class="mf">9.905</span><span class="p">,</span><span class="w"> </span><span class="mf">8.99</span><span class="p">,</span><span class="w"> </span><span class="mf">9.07</span><span class="p">,</span><span class="w"> </span><span class="mf">9.78</span><span class="p">,</span><span class="w"> </span><span class="mf">9.82</span><span class="p">,</span><span class="w"> </span><span class="mf">9.18</span><span class="p">,</span><span class="w"> </span><span class="mf">8.47</span><span class="p">,</span><span class="w"> </span><span class="mf">8.15</span><span class="p">,</span><span class="w"> </span><span class="mf">7.92</span><span class="p">,</span><span class="w"> </span><span class="mf">7.81</span><span class="p">,</span><span class="w">
    </span><span class="mf">8.49</span><span class="p">,</span><span class="w"> </span><span class="mf">8.15</span><span class="p">,</span><span class="w"> </span><span class="mf">7.5</span><span class="p">,</span><span class="w"> </span><span class="mf">8.33</span><span class="p">,</span><span class="w"> </span><span class="mf">7.72</span><span class="p">,</span><span class="w"> </span><span class="mf">9.39</span><span class="p">,</span><span class="w"> </span><span class="mf">9.42</span><span class="p">,</span><span class="w"> </span><span class="mf">9.1</span><span class="p">,</span><span class="w"> </span><span class="mf">8.86</span><span class="p">,</span><span class="w"> </span><span class="mf">8.62</span><span class="p">,</span><span class="w"> </span><span class="mf">8.25</span><span class="p">,</span><span class="w"> </span><span class="mf">8.32</span><span class="p">,</span><span class="w"> </span><span class="mf">8.11</span><span class="p">,</span><span class="w"> </span><span class="mf">8.57</span><span class="p">,</span><span class="w"> </span><span class="mf">9.11</span><span class="p">,</span><span class="w"> </span><span class="mf">9.3</span><span class="p">,</span><span class="w"> </span><span class="mf">9.37</span><span class="p">,</span><span class="w">
    </span><span class="mf">9.22</span><span class="p">,</span><span class="w"> </span><span class="mf">8.86</span><span class="p">,</span><span class="w"> </span><span class="mf">8.87</span><span class="p">,</span><span class="w"> </span><span class="mf">9.14</span><span class="p">,</span><span class="w"> </span><span class="mf">10.63</span><span class="p">,</span><span class="w"> </span><span class="mf">9.695</span><span class="p">,</span><span class="w"> </span><span class="mf">9.785</span><span class="p">,</span><span class="w"> </span><span class="mf">9.25</span><span class="p">,</span><span class="w"> </span><span class="mf">10.31</span><span class="p">,</span><span class="w"> </span><span class="mf">10.58</span><span class="p">,</span><span class="w"> </span><span class="mf">11.55</span><span class="p">,</span><span class="w"> </span><span class="mf">9.865</span><span class="p">,</span><span class="w"> </span><span class="mf">11.64</span><span class="p">,</span><span class="w"> </span><span class="mf">11.54</span><span class="p">,</span><span class="w"> </span><span class="mf">11.85</span><span class="p">,</span><span class="w">
    </span><span class="mf">12.05</span><span class="p">,</span><span class="w"> </span><span class="mf">11.92</span><span class="p">,</span><span class="w"> </span><span class="mf">10.96</span><span class="p">,</span><span class="w"> </span><span class="mf">13.86</span><span class="p">,</span><span class="w"> </span><span class="mf">14.02</span><span class="p">,</span><span class="w"> </span><span class="mf">14.19</span><span class="p">,</span><span class="w"> </span><span class="mf">14.16</span><span class="p">,</span><span class="w"> </span><span class="mf">13.15</span><span class="p">,</span><span class="w"> </span><span class="mf">12.11</span><span class="p">,</span><span class="w"> </span><span class="mf">12.26</span><span class="p">,</span><span class="w"> </span><span class="mf">12.04</span><span class="p">,</span><span class="w"> </span><span class="mf">11.82</span><span class="p">,</span><span class="w"> </span><span class="mf">11.3186</span><span class="w">
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<h3 id="calculating-returns---third-step">Calculating Returns - Third step</h3>
<p>The last step is calling the MCP tool named <code class="language-plaintext highlighter-rouge">calculate_var_cvar_historical_mcp</code> receiving as parameter the returns obtained in second step, together with significance level.</p>

<p><img src="http://localhost:4000/assets/images/04_llm_and_mcp/01_02_calling_mcp_server.png" alt="Calling MCP Server" /></p>

<p>Input below:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"returns"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="mf">-0.009708737864077678</span><span class="p">,</span><span class="w"> </span><span class="mf">0.029411764705882377</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.028571428571428595</span><span class="p">,</span><span class="w"> </span><span class="mf">0.00980392156862746</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.019417475728155355</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.028571428571428595</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.032941176470588196</span><span class="p">,</span><span class="w"> </span><span class="mf">0.05433901054339008</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.057692307692307744</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.10204081632653059</span><span class="p">,</span><span class="w"> </span><span class="mf">0.08306818181818175</span><span class="p">,</span><span class="w"> </span><span class="mf">0.011856048683244243</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.07019908751555369</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.010482881677261028</span><span class="p">,</span><span class="w"> </span><span class="mf">0.1588124931023066</span><span class="p">,</span><span class="w"> </span><span class="mf">0.019047619047619063</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.06663551401869161</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.049664563933113026</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.011484564324096417</span><span class="p">,</span><span class="w"> </span><span class="mf">0.03333333333333337</span><span class="p">,</span><span class="w"> </span><span class="mf">0.04838709677419359</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.03846153846153849</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.0555555555555556</span><span class="p">,</span><span class="w"> </span><span class="mf">0.058823529411764754</span><span class="p">,</span><span class="w"> </span><span class="mf">0.10185185185185174</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0756302521008404</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.08593750000000007</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.06837606837606826</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.04587155963302756</span><span class="p">,</span><span class="w"> </span><span class="mf">0.009615384615384623</span><span class="p">,</span><span class="w"> </span><span class="mf">0.06666666666666672</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.08928571428571436</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.01960784313725492</span><span class="p">,</span><span class="w"> </span><span class="mf">0.028846153846153872</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.03738317757009349</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.03844660194174765</span><span class="p">,</span><span class="w"> </span><span class="mf">0.03998384491114709</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.07961165048543696</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.05358649789029531</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.11268390548372711</span><span class="p">,</span><span class="w"> </span><span class="mf">0.06293179248838075</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.08083195462065695</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.06775520699408584</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.04852498494882601</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0803594026828651</span><span class="p">,</span><span class="w"> </span><span class="mf">0.050251844910389996</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.06212357796118668</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.05315733143061015</span><span class="p">,</span><span class="w"> </span><span class="mf">0.014227642276422701</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.03139612558450234</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.03908045977011498</span><span class="p">,</span><span class="w"> </span><span class="mf">0.10944976076555027</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.022318059299191312</span><span class="p">,</span><span class="w"> </span><span class="mf">0.06517423908248779</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.020706077233668102</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.03795327201606928</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.05505494505494509</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.02418885916967094</span><span class="p">,</span><span class="w"> </span><span class="mf">0.04053593732258425</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.09297250109122647</span><span class="p">,</span><span class="w"> </span><span class="mf">0.004571703561116355</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.041916167664670566</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.057999999999999996</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.08691613588110403</span><span class="p">,</span><span class="w"> </span><span class="mf">0.06612410986775176</span><span class="p">,</span><span class="w"> </span><span class="mf">0.09651035986913846</span><span class="p">,</span><span class="w"> </span><span class="mf">0.019641969169567425</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.03950256035113385</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.03635937133474083</span><span class="p">,</span><span class="w"> </span><span class="mf">0.022882181110029258</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.02427415516420762</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.005975609756097445</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.026009078640657584</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.0364025695931478</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.006535947712418306</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.02605263157894742</span><span class="p">,</span><span class="w"> </span><span class="mf">0.05120237773574716</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.04125433748875468</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.03344853122685758</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.038436981228451045</span><span class="p">,</span><span class="w"> </span><span class="mf">0.015139442231075651</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.02145473574044999</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.04933155080213909</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.0002548095298765308</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.007391359755320398</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.016690204134035193</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.013578796187491938</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.08060886829913974</span><span class="p">,</span><span class="w"> </span><span class="mf">0.03160215580597738</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.03763951555450006</span><span class="p">,</span><span class="w"> </span><span class="mf">0.13510178901912404</span><span class="p">,</span><span class="w"> </span><span class="mf">0.03369565217391295</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.16719242902208217</span><span class="p">,</span><span class="w"> </span><span class="mf">0.1531531531531531</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.039062500000000035</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.016260162601626032</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.008264462809917363</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.016666666666666684</span><span class="p">,</span><span class="w"> </span><span class="mf">0.2295081967213115</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.02666666666666669</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.034246575342465786</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.06382978723404245</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.06818181818181823</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.08130081300813015</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.008849557522123706</span><span class="p">,</span><span class="w"> </span><span class="mf">0.07142857142857129</span><span class="p">,</span><span class="w"> </span><span class="mf">0.19166666666666665</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.055944055944056</span><span class="p">,</span><span class="w"> </span><span class="mf">0.006622516556291397</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.019736842105263174</span><span class="p">,</span><span class="w"> </span><span class="mf">0.14093959731543623</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.08823529411764701</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.09032258064516137</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.07801418439716304</span><span class="p">,</span><span class="w"> </span><span class="mf">0.007692307692307699</span><span class="p">,</span><span class="w"> </span><span class="mf">0.030534351145038195</span><span class="p">,</span><span class="w"> </span><span class="mf">0.09629629629629621</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.17567567567567569</span><span class="p">,</span><span class="w"> </span><span class="mf">0.5804597701149425</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.19999999999999993</span><span class="p">,</span><span class="w"> </span><span class="mf">0.09090909090909079</span><span class="p">,</span><span class="w"> </span><span class="mf">0.2708333333333333</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.009836065573770428</span><span class="p">,</span><span class="w"> </span><span class="mf">0.03973509933774838</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.009554140127388614</span><span class="p">,</span><span class="w"> </span><span class="mf">0.022508038585209094</span><span class="p">,</span><span class="w"> </span><span class="mf">0.37735849056603765</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.02054794520547942</span><span class="p">,</span><span class="w"> </span><span class="mf">0.45190156599552583</span><span class="p">,</span><span class="w"> </span><span class="mf">0.13713405238828963</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.1910569105691057</span><span class="p">,</span><span class="w"> </span><span class="mf">0.19932998324958132</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.1773743016759776</span><span class="p">,</span><span class="w"> </span><span class="mf">0.32028469750889693</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.039532794249775495</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.3012160898035547</span><span class="p">,</span><span class="w"> </span><span class="mf">0.25435073627844706</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.16969050160085397</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0355839416058393</span><span class="p">,</span><span class="w"> </span><span class="mf">0.360352422907489</span><span class="p">,</span><span class="w"> </span><span class="mf">0.1062176165803108</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.00468384074941442</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.1023529411764706</span><span class="p">,</span><span class="w"> </span><span class="mf">0.3106159895150721</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.049</span><span class="p">,</span><span class="w"> </span><span class="mf">0.025762355415352364</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.057406458226550536</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.4540511147362698</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.11055776892430275</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.322508398656215</span><span class="p">,</span><span class="w"> </span><span class="mf">0.4793388429752065</span><span class="p">,</span><span class="w"> </span><span class="mf">0.22234636871508384</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.027422303473491838</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.12544483985765126</span><span class="p">,</span><span class="w"> </span><span class="mf">0.422177009155646</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.005007153075822624</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.031631919482386736</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.020044543429844196</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.05681818181818182</span><span class="p">,</span><span class="w"> </span><span class="mf">0.05060240963855428</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.03211009174311926</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.028436018957345925</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.07073170731707311</span><span class="p">,</span><span class="w"> </span><span class="mf">0.022779043280182286</span><span class="p">,</span><span class="w"> </span><span class="mf">0.01855976243504083</span><span class="p">,</span><span class="w"> </span><span class="mf">0.00801749271137022</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.039045553145336295</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.033107599699021786</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.038910505836575876</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.1076923076923077</span><span class="p">,</span><span class="w"> </span><span class="mf">0.06624319419237754</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0425531914893617</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.035918367346938734</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.10922946655376807</span><span class="p">,</span><span class="w"> </span><span class="mf">0.04847908745247147</span><span class="p">,</span><span class="w"> </span><span class="mf">0.039891205802357325</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.06277244986922412</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.05860465116279077</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.10770750988142291</span><span class="p">,</span><span class="w"> </span><span class="mf">0.027685492801771874</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.13469827586206898</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0535491905354921</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.08983451536643033</span><span class="p">,</span><span class="w"> </span><span class="mf">0.020779220779220797</span><span class="p">,</span><span class="w"> </span><span class="mf">0.040712468193384144</span><span class="p">,</span><span class="w"> </span><span class="mf">0.04034229828850857</span><span class="p">,</span><span class="w"> </span><span class="mf">0.09870740305522913</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.15401069518716573</span><span class="p">,</span><span class="w"> </span><span class="mf">0.01769911504424786</span><span class="p">,</span><span class="w"> </span><span class="mf">0.11180124223602465</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.02234636871508372</span><span class="p">,</span><span class="w"> </span><span class="mf">0.28228571428571436</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.0053475935828877445</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.0806451612903226</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.0346003898635478</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.09237758707723365</span><span class="p">,</span><span class="w"> </span><span class="mf">0.008898776418242499</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.07828004410143319</span><span class="p">,</span><span class="w"> </span><span class="mf">0.004089979550102344</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.06517311608961308</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.07734204793028313</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.037780401416765086</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.028220858895705574</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.01388888888888893</span><span class="p">,</span><span class="w"> </span><span class="mf">0.08706786171574912</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.04004711425206123</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.0797546012269939</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.11066666666666668</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.07322929171668671</span><span class="p">,</span><span class="w"> </span><span class="mf">0.21632124352331616</span><span class="p">,</span><span class="w"> </span><span class="mf">0.00319488817891367</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.033970276008492596</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.026373626373626398</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.027088036117381517</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.04292343387470989</span><span class="p">,</span><span class="w"> </span><span class="mf">0.008484848484848519</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.025240384615384717</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.05672009864364992</span><span class="p">,</span><span class="w"> </span><span class="mf">0.06301050175029162</span><span class="p">,</span><span class="w"> </span><span class="mf">0.020856201975850856</span><span class="p">,</span><span class="w"> </span><span class="mf">0.007526881720429947</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.01600853788687285</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.03904555314533635</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0011286681715575381</span><span class="p">,</span><span class="w"> </span><span class="mf">0.030439684329199704</span><span class="p">,</span><span class="w"> </span><span class="mf">0.16301969365426697</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.08795860771401698</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.009283135636926235</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.05467552376085847</span><span class="p">,</span><span class="w"> </span><span class="mf">0.11459459459459465</span><span class="p">,</span><span class="w"> </span><span class="mf">0.026188166828321976</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0916824196597354</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.14588744588744593</span><span class="p">,</span><span class="w"> </span><span class="mf">0.1799290420679169</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.008591065292096342</span><span class="p">,</span><span class="w"> </span><span class="mf">0.026863084922010443</span><span class="p">,</span><span class="w"> </span><span class="mf">0.016877637130801777</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.010788381742738653</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.08053691275167778</span><span class="p">,</span><span class="w"> </span><span class="mf">0.2645985401459853</span><span class="p">,</span><span class="w"> </span><span class="mf">0.011544011544011554</span><span class="p">,</span><span class="w"> </span><span class="mf">0.012125534950071322</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.0021141649048625343</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.07132768361581919</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.07908745247148295</span><span class="p">,</span><span class="w"> </span><span class="mf">0.012386457473162705</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.017944535073409516</span><span class="p">,</span><span class="w">
        </span><span class="mf">-0.018272425249169343</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.04241962774957701</span><span class="w">
    </span><span class="p">],</span><span class="w">
    </span><span class="nl">"significance_level"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.05</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<h2 id="results">Results</h2>
<p>As result the VaR is express by <code class="language-plaintext highlighter-rouge">GPT 4.1</code>. I should confess that in this ocassion I expected to see the VaR in percentage format, but LLM model did not interpret it in that way. Instead, the decimal format was shown, such that the HVaR calculated given the functions above, is about -10.77% and HCVaR is about -18.9%. Result is shown below.</p>

<p><img src="http://localhost:4000/assets/images/04_llm_and_mcp/01_03_calling_mcp_server.png" alt="Calling MCP Server" /></p>

<p>Moreover, a checking was performed by running the functions in a jupyter notebook calling the same functions in the same order. Result is shown in my Github repositoy, <a href="https://github.com/craquinterogo/quantitative_finance/blob/master/LLM/mcp-server/VaR_validation.ipynb" target="_blank" rel="noopener">here</a>. As expected the result is the same as displayed in Github Copilot chat.</p>

<h1 id="takeaways">Takeaways</h1>
<p>The process of creating tools is fairly straightforward. However, designing libraries that expose functions, classes and modules via an MCP server appears to offer advantages. There is no doubt about the potential usefulness of exposing existing modules via LLM models using MCP servers. The challenge therefore lies in structuring an effective prompt that enables the agent to develop a robust plan for executing tools in the expected order. Prompt engineering is therefore essential, particularly when a large number of tools are deployed.</p>

<p>Similarly, strategically designing tools helps to avoid confusing the agent. Tools with similar names or descriptions may produce undesirable results.</p>
</p>
        
			
		
			
        
			
		
			
        
			
		
			
        
			
		
			
        
			
		
			
        
			
		
			
        
			
		
	</div>
      </section>

    </div>

    <!-- Contact -->
<section id="contact">
	<div class="inner">
		<section>
			<form action="https://formspree.io/craquinterogo@gmail.com" method="POST">
				<div class="field half first">
					<label for="name">Name</label>
					<input type="text" name="name" id="name" />
				</div>
				<div class="field half">
					<label for="email">Email</label>
					<input type="text" name="_replyto" id="email" />
				</div>
				<div class="field">
					<label for="message">Message</label>
					<textarea name="message" id="message" rows="6"></textarea>
				</div>
				<ul class="actions">
					<li><input type="submit" value="Send Message" class="special" /></li>
					<li><input type="reset" value="Clear" /></li>
				</ul>
			</form>
		</section>
		<section class="split">
			<section>
				<div class="contact-method">
					<span class="icon alt fa-envelope"></span>
					<h3>Email</h3>
					<a href="#">craquinterogo@gmail.com</a>
				</div>
			</section>
			<section>
				<div class="contact-method">
					<span class="icon alt fa-phone"></span>
					<h3>Phone</h3>
					<span>(+48) 539 641 920</span>
				</div>
			</section>
			<section>
				<div class="contact-method">
					<span class="icon alt fa-home"></span>
					<h3>Address</h3>
					<span>
					
					    Warsaw<br />
					
					
					    Warsaw,
					
					
					    WAR 
					
					
					
					    Poland
					
					</span>
				</div>
			</section>
		</section>
	</div>
</section>

<!-- Footer -->
	<footer id="footer">
		<div class="inner">
			<ul class="icons">
				
				<li><a href="https://twitter.com/_arleyquintero" class="icon alt fa-twitter" target="_blank"><span class="label">Twitter</span></a></li>
				
				
				
				
				
				
				
				
				<li><a href="https://github.com/craquinterogo/quantitative_finance" class="icon alt fa-github" target="_blank"><span class="label">GitHub</span></a></li>
				
				
				
				<li><a href="https://www.linkedin.com/in/craquinterogo/?locale=en_US" class="icon alt fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
				
			</ul>
			<ul class="copyright">
				<li>&copy; Cristian Quintero</li>
				<li>Design: <a href="https://html5up.net" target="_blank">HTML5 UP</a></li>
				<li>Jekyll integration: <a href="http://andrewbanchi.ch" target="_blank">Andrew Banchich</a></li>

			</ul>
		</div>
	</footer>

</div>

<!-- Scripts -->
	<script src="http://localhost:4000/assets/js/jquery.min.js"></script>
	<script src="http://localhost:4000/assets/js/jquery.scrolly.min.js"></script>
	<script src="http://localhost:4000/assets/js/jquery.scrollex.min.js"></script>
	<script src="http://localhost:4000/assets/js/skel.min.js"></script>
	<script src="http://localhost:4000/assets/js/util.js"></script>
	<!--[if lte IE 8]><script src="http://localhost:4000/assets/js/ie/respond.min.js"></script><![endif]-->
	<script src="http://localhost:4000/assets/js/main.js"></script>


        <script>
      // Sidebar toggle functionality
      document.addEventListener('DOMContentLoaded', function() {
        const sidebar = document.getElementById('sidebar');
        const sidebarToggle = document.getElementById('sidebar-toggle');
        const sidebarOverlay = document.getElementById('sidebar-overlay');
        const tocSidebar = document.getElementById('toc-sidebar');
        const tocToggle = document.getElementById('toc-toggle');
        
        if (sidebarToggle && sidebar && sidebarOverlay) {
          sidebarToggle.addEventListener('click', function() {
            sidebar.classList.toggle('active');
            sidebarOverlay.classList.toggle('active');
          });
          
          sidebarOverlay.addEventListener('click', function() {
            sidebar.classList.remove('active');
            sidebarOverlay.classList.remove('active');
          });
        }
        // TOC toggle button logic
        if (tocToggle && tocSidebar) {
          // Always hide TOC sidebar on load
          tocSidebar.classList.add('toc-hidden');
          tocToggle.addEventListener('click', function() {
            tocSidebar.classList.toggle('toc-hidden');
            tocSidebar.classList.toggle('active');
          });
        }
        // --- Table of Contents Sidebar Generation with Numbering (skip first heading in TOC and content, TOC starts at 1) ---
        const mainContent = document.querySelector('#main .inner');
        if (tocSidebar && mainContent) {
          const headings = mainContent.querySelectorAll('h1, h2, h3, h4');
          if (headings.length > 2) {
            let tocHtml = '<div class="toc-title">Table of Contents</div><ul class="toc-list">';
            let numbers = [0, 0, 0, 0]; // For h1-h4
            let tocNumbers = [0, 0, 0, 0]; // Separate numbering for TOC
            // Start from the second heading (index 1)
            for (let idx = 1; idx < headings.length; idx++) {
              const heading = headings[idx];
              const level = parseInt(heading.tagName.substring(1)) - 1; // 0-based
              // Update numbering for content
              numbers[level]++;
              for (let i = level + 1; i < numbers.length; i++) numbers[i] = 0;
              // Build number string for content
              let numberStr = numbers.slice(0, level + 1).filter(n => n > 0).join('.');
              // Prepend number to heading text
              let headingText = heading.textContent.replace(/^\d+(\.\d+)*\.\s*/, '');
              heading.innerHTML = `<span class=\"section-number\">${numberStr}.</span> ` + headingText;
              if (!heading.id) {
                heading.id = headingText.trim().toLowerCase().replace(/[^a-z0-9]+/g, '-');
              }
              // Only add to TOC if not the first heading after the title (i.e., idx > 1)
              if (idx > 1) {
                // Update TOC numbering
                tocNumbers[level]++;
                for (let i = level + 1; i < tocNumbers.length; i++) tocNumbers[i] = 0;
                let tocNumberStr = tocNumbers.slice(0, level + 1).filter(n => n > 0).join('.');
                tocHtml += `<li class=\"toc-level-${level+1}\"><a href=\"#${heading.id}\"><span class='section-number'>${tocNumberStr}.</span> ${headingText}</a></li>`;
              }
            }
            tocHtml += '</ul>';
            tocSidebar.innerHTML = tocHtml;
            // Do NOT add .active or remove .toc-hidden here
          } else {
            tocSidebar.classList.add('toc-hidden'); // Hide if not enough headings
          }
        }
      });
    </script>

  </body>

</html>
