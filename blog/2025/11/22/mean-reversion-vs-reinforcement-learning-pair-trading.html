<!DOCTYPE html>
<!--
    Forty by HTML5 UP
    html5up.net | @ajlkn
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
  -->
<html>

  <head>
	<!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-6YNFJY0416"></script>
	<script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());

	gtag('config', 'G-6YNFJY0416');
	</script>
	<title>Mean Reversion Model vs RL Model in a Pair Trading Strategy: A Comparative Study | </title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="/assets/css/main.css" />
	<!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->
	<link rel="apple-touch-icon" sizes="180x180" href="/assets/images/favicon_io/facapple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicon_io/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicon_io/favicon-16x16.png">
	<link rel="manifest" href="/assets/images/favicon_io/site.webmanifest">
		<!-- KaTeX Configuration -->
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
		onload="renderMathInElement(document.body, {
			delimiters: [
				{left: '$$', right: '$$', display: true},
				{left: '$', right: '$', display: false},
				{left: '\\(', right: '\\)', display: false},
				{left: '\\[', right: '\\]', display: true}
			]
		});"></script>
</head>


  <body class="layout-post">

    <!-- Wrapper -->
<div id="wrapper">

<!-- Header -->
<header id="header">
	<a href="/" class="logo"><strong>Cristian</strong> <span>Quintero</span></a>
	<nav>
		<a href="#menu">Menu</a>
	</nav>
</header>

<!-- Menu -->
<nav id="menu">
	<ul class="links">
        
		    
		
		    
		
		    
		
		    
		        <li><a href="/">Home</a></li>
	    	
		
		    
		
		    
		
		    
		
		
		    
		
		    
		        <li><a href="/about-me">About Me</a></li>
		    
		
		    
		        <li><a href="/all_posts.html">All posts</a></li>
		    
		
		    
		
		    
		        <li><a href="/my-posts/">My posts</a></li>
		    
		
		    
		        <li><a href="/what-I-like/">I like</a></li>
		    
		
	</ul>
	<!-- <ul class="actions vertical">
		<li><a href="#" class="button special fit">Get Started</a></li>
		<li><a href="#" class="button fit">Log In</a></li>
	</ul> -->
</nav>
 
    
    <!-- Enhanced Sidebar with TOC and Category-specific Posts -->
    
      
        
      
    
      
        
          
          
          
    
    <aside id="sidebar" class="sidebar">
      <div class="sidebar-content">
        <!-- Table of Contents Section (Collapsible) -->
        <div class="sidebar-section toc-section">
          <div class="sidebar-section-header" data-target="toc-content">
            <h4>
              <i class="fa fa-list-ul"></i> Table of Contents
            </h4>
            <button class="collapse-toggle" aria-label="Toggle Table of Contents">
              <i class="fa fa-chevron-down"></i>
            </button>
          </div>
          <div id="toc-content" class="sidebar-section-content expanded">
            <nav id="toc-sidebar-nav"></nav>
          </div>
        </div>

        <!-- Related Posts Section (Collapsible) -->
        <div class="sidebar-section related-posts-section">
          <div class="sidebar-section-header" data-target="related-posts-content">
            <h4>
              <i class="fa fa-folder"></i> 
              
                Algorithmic Trading Posts
              
            </h4>
            <button class="collapse-toggle" aria-label="Toggle Related Posts">
              <i class="fa fa-chevron-down"></i>
            </button>
          </div>
          <div id="related-posts-content" class="sidebar-section-content expanded">
            
              <ul class="recent-posts">
                
                
                
                  <li class="recent-post-item">
                    <a href="/blog/2025/04/21/pair-trading-with-reinforcement-learning" class="recent-post-link">
                      
                        <div class="recent-post-image">
                          <img src="/assets/images/02_algo_trading/001_rl_papers/rl_image.jpeg" alt="Pair trading strategy using reinforcement learning algortihms (PPO and A2C)" />
                        </div>
                      
                      <div class="recent-post-content">
                        <h5>Pair trading strategy using reinforce...</h5>
                        
                          <span class="recent-post-date">Apr 21, 2025</span>
                        
                      </div>
                    </a>
                  </li>
                
                
              </ul>
              
              <!-- Show link to all posts in this category -->
              <div class="sidebar-category-link">
                <a href="/my-posts" class="view-all-category">
                  View All Algorithmic Trading Posts →
                </a>
              </div>
            
          </div>
        </div>
      </div>
    </aside>

    <!-- Sidebar toggle for mobile -->
    <button id="sidebar-toggle" class="sidebar-toggle" aria-label="Toggle sidebar">
      <i class="fa fa-bars"></i>
    </button>
    
    <!-- Sidebar overlay for mobile -->
    <div id="sidebar-overlay" class="sidebar-overlay"></div>

    <!-- Main -->
    <div id="main" class="alt">

      <!-- One -->
      <section id="one">
	<div class="inner">
		
			
        
			
		
			
        
          <header class="major">
            <h1>Mean Reversion Model vs RL Model in a Pair Trading Strategy: A Comparative Study</h1>
          </header>
          
            <div class="post-image-container">
              <img src="/assets/images/01_post_images/001_beginning/001_reingorcement_learning.jpg" alt="Mean Reversion Model vs RL Model in a Pair Trading Strategy: A Comparative Study" />
            </div>
          
          <p>Nov 22, 2025</p>
          <p><p>This weekend I was motivated to show a bit of my experiments influenced by the refreshing feeling of sharing a bit of learning process last 5 years in algorithmic trading. Thereby, I want to show you a model I was working on as a piece of personal research that subsequently gave rise to another related article named <a href="https://link.springer.com/epdf/10.1007/s42979-025-03854-0?sharing_token=sGsNKhjDnd_2ZBoSrB-eGPe4RwlQNchNByi7wbcMAY7scbCEPPediauUhVR8XxKT4_zdALps152Vi-Z1NBxIhLio_PphS-G3Iut6fLamaFblG8cBAUWDVZqHXO5UBGcPR4OizeqWiIHNGx9DPfY2TVgQfTs5nLkRXZVxR70kbBc%3D" target="_blank" rel="noopener">Deep Reinforcement Learning in Continuous Action Spaces for Pair Trading: A Comparative Study of A2C and PPO</a>, where I’m a coautor. This work is the anteroom to the continuous variable experiment using Reinforcement Learning (RL), but recently touched again as an experimental exercise. <strong>The difference will be noticed by the conscious reader in the subset of asset classes, methodology and RL models</strong>.</p>

<div style="background-color: #fff3cd; color: gray; border-left: 6px solid #ffe066; padding: 1em; margin-bottom: 1em;">
<strong style="color: red;">⚠️ Experimental Notice:</strong><br />
This article presents ongoing updated version of research and is currently <strong style="color: red;">under construction/verification of the codes</strong>. Results, methodologies, and conclusions may be updated as the updated experiment progresses. Feedback and suggestions are welcome!
</div>

<h1 id="context">Context</h1>

<p>A bit of context is that pair trading is a market-neutral strategy used in trading desks across various asset classes. The core idea is to profit from temporary deviations in the price relationship between two related assets. According to the <strong>Adaptive Market Hypothesis (AMH)</strong>, see <a class="citation" href="#Lo">(Lo, 2004)</a>, markets incorporate factors beyond pure rationality, preventing deterministic dynamics and opening the door to stochastic modeling approaches.</p>

<p>Consequently, this experiment explores whether reinforcement learning can outperform traditional stochastic mean reversion models in identifying optimal entry levels for pair trading strategies.</p>

<h2 id="research-questions">Research Questions</h2>

<ol>
  <li>Are there benefits on using RL models in an asset pair whose spread/signal is stable around the mean for a given time horizon?</li>
  <li>Are the optimal entry levels of each model significantly different?</li>
</ol>

<h1 id="theoretical-framework">Theoretical Framework</h1>

<h2 id="pair-trading-strategy">Pair Trading Strategy</h2>

<p>Pair trading is also called <strong>statistical arbitrage</strong> for some authors <a class="citation" href="#Isichenko">(Isichenko, 2021; Pole, 2007)</a>, that may be defined as a market-neutral. Thereby, it is a strategy where profits are sought by arbitraging temporary deviations in the price relationship of a pair of assets. Fun fact, <a class="citation" href="#Pole">(Pole, 2007)</a> says, citing Dowdy, that <em>“statistical arbitrage was invented, curiously, despite the lack of statisticians or statistical content of much of the work”</em>. However, let’s work with the current tools. 
The spread signal is typically defined as shown in the table below, where $\beta$ depicts the number of units of asset $B$ required to equal the price level $S_A(t)$ associated to asset $A$, given that price of $B$ is $S_B(t)$ at time $t$.</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Signal</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Subtraction method</td>
      <td>$S_A(t) - \beta_1 \cdot S_B(t)$</td>
      <td>Difference between asset prices, scaled by $\beta$. Prefered when econometrics is used for calibration.</td>
    </tr>
    <tr>
      <td>Ratio method</td>
      <td>$\frac{S_A(t)}{\beta_2 \cdot S_B(t)}$</td>
      <td>Ratio of asset prices. Prefered when calibration method is optimized around 1.</td>
    </tr>
  </tbody>
</table>

<p><strong>Example:</strong><br />
Suppose asset $A$ is trading at 100 monetary units (m.u.) and asset $B$ at 50 m.u. If $\beta = 2$, then $2 \times S_B = S_A$, meaning two units of $B$ match the price of one unit of $A$.</p>
<ul>
  <li><strong>Subtraction method:</strong> $S_A - \beta S_B = 100 - 2 \times 50 = 0$</li>
  <li><strong>Ratio method:</strong> $\frac{S_A}{\beta S_B} = \frac{100}{2 \times 50} = 1$</li>
</ul>

<p>This illustrates how $\beta$ normalizes the spread signal, allowing meaningful comparison and trading decisions between assets with different price scales.</p>

<h3 id="selected-signal">Selected signal</h3>

<p>For this study, the spread model was defined using logarithmic prices:</p>

\[X(t) = \ln(S_A(t)) - \beta \cdot \ln(S_B(t))\]

<p>where $S_i(t)$ is the quote of asset $i$, $\beta &gt; 0$ represents the investment ratio between long and short positions, and since the conversion factor is 1:1 for the analyzed assets, $\beta = 1$.</p>

<h2 id="ornstein-uhlenbeck-mean-reversion-model">Ornstein-Uhlenbeck Mean Reversion Model</h2>

<p>The Ornstein-Uhlenbeck (OU) process is one of the most recognized mean reversion models. Introduced by physicists Leonard Salomon Ornstein and George Eugene Uhlenbeck in 1930, it models the effect of friction forces proportional to pressure.</p>

<p>The stochastic differential equation (SDE) for the OU process with Lévy jumps is:</p>

\[dX(t) = -\rho \cdot (X(t) - \theta) \cdot dt + \sigma \cdot dL(t)\]

<p>Where:</p>
<ul>
  <li>$\rho$ is the mean reversion speed parameter</li>
  <li>$\theta$ is the long-term average</li>
  <li>$\sigma$ is the process volatility</li>
  <li>$dL(t)$ is a Lévy process (replacing the traditional Brownian motion $dB$ to account for heavy tails and jumps)</li>
</ul>

<h3 id="key-properties">Key Properties</h3>

<ul>
  <li>The increment of $X$ is subject to a deterministic component $\mu(\theta-X_t)dt$, known as <strong>drift</strong></li>
  <li>The increment is also subject to a diffusion component $\sigma dL_t$, which is stochastic.</li>
  <li>When $X_t$ is greater/less than $\theta$, a negative/positive effect is imprinted on $dX_t$, causing the next element of $X$ to be lower/higher than the current one</li>
</ul>

<h3 id="conditional-density-function">Conditional Density Function</h3>

<p>Based on <a class="citation" href="#Leung">(Leung &amp; Li, 2016)</a>, the conditional density function of $X_t$ at time $t_i$ with increments $\Delta t = t_i - t_{i-1}$ is:</p>

\[f^{OU}(x_i|x_{i-1};\theta,\mu, \sigma) = \frac{1}{\sqrt{2\pi\widetilde{\sigma}^2}}\exp\left(- \frac{(x_i - x_{i-1}e^{-\mu\Delta t}-\theta (1-e^{-\mu\Delta t}))^2}{2\widetilde{\sigma}^2} \right)\]

<p>where:</p>

\[\widetilde{\sigma}^2 = \sigma^2\frac{1-e^{-2\mu\Delta t}}{2\mu}\]

<h2 id="generalized-hyperbolic-distribution-ghyp">Generalized Hyperbolic Distribution (GHYP)</h2>

<p>The generalized hyperbolic family of probability distributions is defined as a mixture between a multivariate normal distribution and a generalized inverse Gaussian (GIG) distribution, see <a class="citation" href="#Wibel">(Weibel et al., 2022)</a>. A vector $X$ follows a generalized hyperbolic distribution if:</p>

\[X \stackrel{d}{=} \mu + W\beta + \sqrt{W}AZ\]

<p>where:</p>
<ul>
  <li>$Z \stackrel{d}{=} N_k(0,I_k)$</li>
  <li>$A \in \mathbb{R}^{d \times k}$</li>
  <li>$\mu, \beta \in \mathbb{R}^d$</li>
  <li>$W \geq 0$ is a random vector independent of $Z$ with probability distribution $GIG(\lambda, \xi, \psi)$</li>
</ul>

<h3 id="density-function">Density Function</h3>

<p>The GHYP density function is:</p>

\[f_X(x) = \frac{(\sqrt{\psi/\chi})^\lambda (\psi+\beta' \Sigma^{-1}\beta)^{\frac{d}{2}-\lambda}}{(2\pi)^{\frac{d}{2}} |\Sigma|^{\frac{1}{2}}}  \frac{K_{\lambda-\frac{d}{2}}(\sqrt{(\chi+Q(x))(\psi+\beta' \Sigma^{-1}\beta)})e^{(x-\mu)' \Sigma^{-1}\beta}}{(\sqrt{(\chi+Q(x))(\psi+\beta' \Sigma^{-1} \beta)})^{\frac{d}{2}-\lambda}}\]

<h3 id="special-cases">Special Cases</h3>
<ul>
  <li><strong>Multivariate hyperbolic</strong>: When $\lambda = \frac{d+1}{2}$</li>
  <li><strong>Normal Inverse Gaussian (NIG)</strong>: When $\lambda = \frac{1}{2}$</li>
  <li><strong>Variance-Gamma (VG)</strong>: When $\chi=0$ and $\lambda &gt; 0$</li>
  <li><strong>Generalized hyperbolic t-student</strong>: When $\psi=0$ and $\lambda&lt;0$</li>
</ul>

<h3 id="parameters-estimation">Parameters Estimation</h3>
<p>Regarding the estimation of GHYP distribution parameters: in the multivariate case a modified Expectation–Maximization (EM) algorithm is used, known as Multi-cycle Expectation Conditional Maximization (MCECM). For the univariate GHYP distribution, parameter estimation is performed via maximum log-likelihood.</p>

<p>A relevant feature of these algorithms is that they are commonly employed to estimate parameters of probability distributions when the observed data are incomplete, see <a class="citation" href="#Dempster">(Dempster et al., 1977)</a>; notably, they rely on an iterative procedure.</p>

<h2 id="reinforcement-learning">Reinforcement Learning</h2>

<p>Reinforcement learning (RL) is a method for learning to act by mapping environmental situations ($S_t$) to response actions ($a_t$), maximizing a numerical reward value. Unlike supervised learning, RL doesn’t receive examples classified as correct or incorrect; instead, its learning mechanism operates through positive and negative rewards—learning through trial and error.</p>

<h3 id="rl-method-classification">RL Method Classification</h3>

<p>According to Wu and Konda, RL methods are classified into three categories:</p>

<ol>
  <li><strong>Critic-only</strong>: Methods that rely on value function approximation, oriented toward learning an approximate solution to the Bellman equation</li>
  <li><strong>Actor-only</strong>: Methods that rely on a family of previously parameterized policies, using the performance gradient to update the improvement direction</li>
  <li><strong>Actor-Critic</strong>: Methods that take advantage of both, where the critic part uses value function approximation, which is subsequently updated by the actor’s policy parameters</li>
</ol>

<h3 id="key-components">Key Components</h3>

<ul>
  <li><strong>Policy ($\pi_t$)</strong>: The mechanism by which the agent learns from its environment—the mapping of possible states to possible actions</li>
  <li><strong>Reward signal ($r_t$)</strong>: The objective in the RL process, represented by a number indicating whether a decision was positive or negative</li>
  <li><strong>Value function ($\nu_{\pi}$ or $Q_{\pi}$)</strong>: The long-term reward level</li>
  <li><strong>Environment model</strong>: Determines the dynamics of the environment, used for planning</li>
</ul>

<h3 id="value-function">Value Function</h3>

<p>The value function can be expressed as the expected value of future rewards $r_t$ discounted by $\beta$:</p>

\[\nu_*(a) = \mathbb{E}[R_t|A_t=a]\]

<p>However, since $\nu_*(a)$ is only known with certainty when action $a$ is taken at some future time $t$, the value function must be estimated through function $Q_t(s,a)$:</p>

\[Q_*(s,a) = \max_\pi \mathbb{E}[r_1 + \beta r_{t+1} + \beta^2 r_{t+2}+ ... | s_t = s, A_t = a, \pi]\]

<p>where $\beta$ is the discount factor, $0 \leq \beta \leq 1$.</p>

<h3 id="policy-update-rules">Policy Update Rules</h3>

<p><strong>Q-learning (off-policy):</strong></p>

\[Q(S_t,A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1}+\beta \max_a Q(S_{t+1},a) - Q(S_t,A_t)]\]

<p><strong>SARSA (on-policy):</strong></p>

\[Q(S_t,A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1}+\beta Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)]\]

<p>The key difference is that Q-learning directly considers the action that maximizes the subsequent state, while SARSA only considers the value of the policy applied to the selected action.</p>

<h3 id="rl-algorithms-by-category">RL Algorithms by Category</h3>

<table>
  <thead>
    <tr>
      <th>Category</th>
      <th>Algorithm</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Value-based</td>
      <td>Q-learning, SARSA, DQN</td>
    </tr>
    <tr>
      <td>Policy-based</td>
      <td>REINFORCE, PG, TRPO</td>
    </tr>
    <tr>
      <td>Actor-critic</td>
      <td>DPG, PPO, DDPG, SAC, A2C</td>
    </tr>
    <tr>
      <td>Others</td>
      <td>Model-based RL, Multi-Agent RL</td>
    </tr>
  </tbody>
</table>

<h1 id="model-implementation">Model Implementation</h1>

<h2 id="data-and-methodology">Data and Methodology</h2>

<p>Two liquid ETFs in the local market were selected:</p>
<ul>
  <li><strong>iShares xxxxxxxxx ETF (ABC)</strong></li>
  <li><strong>iShares xxxxxxxxx ETF (XYZ)</strong></li>
</ul>

<p>Both are cross-listed in the US and local markets under the same currency, eliminating the need for currency conversion. The ETFs show different spread dynamics: ABC exhibits stable mean reversion behavior, while XYZ presents temporal shocks in spread levels.</p>

<p><strong>Dataset:</strong></p>
<ul>
  <li>Optimization/Training: 2,370 daily observations (01/02/2012 to 02/01/2021)</li>
  <li>Testing: 345 samples for performance evaluation</li>
  <li>Total: 2,715 observations</li>
</ul>

<h2 id="stochastic-mean-reversion-model">Stochastic Mean Reversion Model</h2>

<h3 id="estimating-ρ-mean-reversion-speed">Estimating ρ (Mean Reversion Speed)</h3>

<p>Following Göncu’s methodology, we construct a variable $\widetilde{X} = X(t) - \bar{X}$, discretize and rewrite the SDE:</p>

\[\Delta \widetilde{X}(t) = - \rho \cdot \widetilde{X}(t) \cdot \Delta t + \sigma \cdot \Delta L(t)\]

\[\widetilde{X}(t+1) = (1-\rho \cdot \Delta t) \cdot \widetilde{X}(t)+\widetilde{\epsilon}(t)\]

<p>where $\widetilde{\epsilon}(t)$ follows a GHYP distribution, independent and identically distributed. We estimate $\Omega = (1-\rho \cdot \Delta t)$ by minimizing the error.</p>

<p><strong>Results:</strong></p>
<ul>
  <li>$\hat{\Omega}_{ABC} = 0.2825$</li>
  <li>$\hat{\Omega}_{XYZ} = 0.9811$</li>
</ul>

<p>Solving for $\rho$:</p>

\[\rho  = \frac{1 - \Omega}{\Delta t}\]

<h3 id="calibrating-the-diffusion-factor">Calibrating the Diffusion Factor</h3>

<p>The residuals $\widetilde{\epsilon}(t)$ show excess kurtosis, justifying the use of a Lévy process with GHYP distribution rather than normal distribution. The parameters were estimated using maximum likelihood:</p>

<table>
  <thead>
    <tr>
      <th>Asset</th>
      <th>$\lambda$</th>
      <th>$\alpha$</th>
      <th>$\chi$</th>
      <th>$\psi$</th>
      <th>$\mu$</th>
      <th>$\sigma$</th>
      <th>$\beta$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ABC</td>
      <td>1</td>
      <td>4.612e-3</td>
      <td>1.06e-5</td>
      <td>2.0017</td>
      <td>5.5251e-4</td>
      <td>0.0102</td>
      <td>-5.477e-4</td>
    </tr>
    <tr>
      <td>XYZ</td>
      <td>1</td>
      <td>1.476e-5</td>
      <td>1.09e-10</td>
      <td>2</td>
      <td>9.451e-4</td>
      <td>0.01279</td>
      <td>-9.4646e-4</td>
    </tr>
  </tbody>
</table>

<h3 id="calibrating-entryexit-levels">Calibrating Entry/Exit Levels</h3>
<p>Entry and exit levels are inspired in <a class="citation" href="#Leung">(Leung &amp; Li, 2016)</a> that treats the process as an arrival times process, following a Poison process.
For a <strong>short position</strong> in the pair, the profit and loss function is defined as:</p>

\[\nu^1(c,T) = \begin{cases}
c &amp; \tau_1\leq T \\
X(0) - X(T) &amp; \tau_1 &gt; T
\end{cases}\]

<p>where $\tau_1 = \inf \{t&gt;0; X(t)=\bar{X} | X(0) = \bar{X}+c \}$ and $c = X(0) - \theta$.</p>

<p>For a <strong>long position</strong>, redefining $c = \theta - X(0)$:</p>

\[\nu^1(c,T) = \begin{cases}
c &amp; \tau_1\leq T \\
X(T) - X(0) &amp; \tau_1 &gt; T
\end{cases}\]

<p>The expected utility is:</p>

\[\mathbb{E}[\nu_1(c,T)] = P(\tau_1&lt;T)c+(1-P(\tau_1&lt;T))(X(0)-\mathbb{E}[X(T)|\tau_1&gt;T])\]

<p>To find the value of $X(T)$, we use the generalized formula:</p>

\[\widetilde{X}_t = \Omega^n \widetilde{X}_0 + \sum_{i=1}^t{\Omega^{t-i} \widetilde{\epsilon}_i}\]

<p><strong>Monte Carlo Simulation:</strong></p>
<ul>
  <li>100,000 simulations with 300 steps each</li>
  <li>Time horizon $T$ up to 250 days (1 trading year)</li>
  <li>Range of $c$ from 1 to observed max/min for short/long positions</li>
</ul>

<p><strong>Optimal Levels Found:</strong></p>
<ul>
  <li>Short position: $c_{short} = 1.017104$</li>
  <li>Long position: $c_{long} = 0.982084$</li>
</ul>

<h2 id="reinforcement-learning-model">Reinforcement Learning Model</h2>

<h3 id="model-components">Model Components</h3>

<ul>
  <li><strong>Action space</strong>: 3 dimensions (Hold, Buy, Sell)</li>
  <li><strong>Observation space</strong>: Historical spread range ± 10%</li>
  <li><strong>Policy update</strong>: Both Q-learning (off-policy) and SARSA (on-policy)</li>
</ul>

<h3 id="reward-function">Reward Function</h3>
<p>Calibrations take into account the comments in <a class="citation" href="#Yuan">(Yuan, 2019)</a> where is affirmed the inclusion of signiticative wrong scenarios help to improve the learning process, in here by scaling the lose by 3. For <strong>short position</strong>:</p>

\[r_t(a_t|\text{Position}=\text{Short}, t &lt; T) = \begin{cases}
x_t - \theta &amp; x_t &gt; \theta \\
-3 (x_t - \theta) &amp; \text{otherwise}
\end{cases}\]

<p>For <strong>long position</strong>:</p>

\[r_t(a_t|\text{Position}=\text{Long}, t &lt; T) = \begin{cases}
\theta - x_t &amp; \theta &gt; x_t \\
-3 (x_t - \theta) &amp; \text{otherwise}
\end{cases}\]

<p>The negative reward has a scaling factor of -3 to create greater impact during the learning process for counterproductive actions.</p>

<h3 id="hyperparameter-optimization">Hyperparameter Optimization</h3>

<p>The delta $\delta$ is defined as the difference between expected and observed values in the Q-value update function:</p>

<p><strong>For Q-learning:</strong></p>

\[\delta_{Q\text{-learning}} = R_{t+1}+\beta \max_a Q(S_{t+1},a) - Q(S_t,A_t)\]

<p><strong>For SARSA:</strong></p>

\[\delta_{SARSA} = R_{t+1}+\beta Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)\]

<p>Analysis of $\alpha$ (learning rate) and $\beta$ (discount factor):</p>
<ul>
  <li>$\alpha$ shows an inverse relationship with $\delta$</li>
  <li>$\beta$ shows a direct relationship with $\delta$</li>
</ul>

<h3 id="optimal-parameters">Optimal Parameters</h3>

<table>
  <thead>
    <tr>
      <th>Position</th>
      <th>$\alpha$</th>
      <th>$\beta$</th>
      <th>Algorithm</th>
      <th>$\delta$</th>
      <th>Episodes for Convergence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Short</td>
      <td>0.8</td>
      <td>0.2</td>
      <td>SARSA</td>
      <td>0.175</td>
      <td>1,000</td>
    </tr>
    <tr>
      <td>Short</td>
      <td>0.8</td>
      <td>0.2</td>
      <td>Q-learning</td>
      <td>0.175</td>
      <td>680</td>
    </tr>
    <tr>
      <td>Long</td>
      <td>0.75</td>
      <td>0.2</td>
      <td>SARSA</td>
      <td>0.25</td>
      <td>3,000</td>
    </tr>
    <tr>
      <td>Long</td>
      <td>0.75</td>
      <td>0.2</td>
      <td>Q-learning</td>
      <td>0.25</td>
      <td>13,000</td>
    </tr>
  </tbody>
</table>

<p><strong>Training Details:</strong></p>
<ul>
  <li>Episodes: 5,000</li>
  <li>Steps per episode: 30</li>
  <li>$\epsilon$-greedy strategy for exploration</li>
  <li>Random selection of time series segments for each episode</li>
</ul>

<h3 id="rl-optimal-levels">RL Optimal Levels</h3>

<p>The optimal levels were selected by finding the state $X_j$ that maximizes the Q-value in the respective column:</p>

\[X_{\text{short/long}}^* = \{ X_j \mid j = \max_i Q_{i,\text{short/long}} \}\]

<p><strong>Results:</strong></p>
<ul>
  <li>Upper threshold (short position): $c_{RL-upper} = 1.022086$</li>
  <li>Lower threshold (long position): $c_{RL-lower} = 0.993484$</li>
</ul>

<h1 id="results-and-comparison">Results and Comparison</h1>

<h2 id="optimal-entry-levels">Optimal Entry Levels</h2>

<table>
  <thead>
    <tr>
      <th>Asset</th>
      <th>Model</th>
      <th>$P_{short}$</th>
      <th>$P_{long}$</th>
      <th>$\theta$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>ABC</strong></td>
      <td>Stochastic Mean Reversion</td>
      <td>1.017104</td>
      <td>0.982084</td>
      <td>1.000292</td>
    </tr>
    <tr>
      <td> </td>
      <td>Reinforcement Learning</td>
      <td>1.022086</td>
      <td>0.993484</td>
      <td> </td>
    </tr>
    <tr>
      <td><strong>XYZ</strong></td>
      <td>Stochastic Mean Reversion</td>
      <td>1.01</td>
      <td>0.9755</td>
      <td>0.9695</td>
    </tr>
    <tr>
      <td> </td>
      <td>Reinforcement Learning</td>
      <td>1.0106</td>
      <td>0.9737</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p><strong>Key Observation:</strong> RL assigns thresholds asymmetrically around the mean $\theta$, with the lower level closer to $\theta$ than in the stochastic model.</p>

<h2 id="trading-rules">Trading Rules</h2>

<ol>
  <li>Since ABC ETF is denominated in USD in both US and local markets, no exchange rate is needed</li>
  <li><strong>Long position</strong>: Sell ABC in US market while simultaneously buying ABC in local market</li>
  <li><strong>Short position</strong>: Buy ABC in US market while simultaneously selling ABC in local market</li>
  <li>All positions liquidated when spread crosses mean value $\theta=1.0002921$</li>
  <li>Multiple positions in the same direction allowed, but no simultaneous long and short positions</li>
  <li>Each operation: 1 share of asset A and B simultaneously</li>
  <li>Forced exit after 30 days if no favorable level reached</li>
</ol>

<h2 id="number-of-positions-opened">Number of Positions Opened</h2>

<table>
  <thead>
    <tr>
      <th>Asset</th>
      <th>Model</th>
      <th>Short Operations</th>
      <th>Long Operations</th>
      <th>Total</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>ABC</strong></td>
      <td>Stochastic Mean Reversion</td>
      <td>9</td>
      <td>12</td>
      <td>21</td>
    </tr>
    <tr>
      <td> </td>
      <td>Reinforcement Learning</td>
      <td>4</td>
      <td>70</td>
      <td>74</td>
    </tr>
    <tr>
      <td><strong>XYZ</strong></td>
      <td>Stochastic Mean Reversion</td>
      <td>124</td>
      <td>115</td>
      <td>139</td>
    </tr>
    <tr>
      <td> </td>
      <td>Reinforcement Learning</td>
      <td>124</td>
      <td>115</td>
      <td>139</td>
    </tr>
  </tbody>
</table>

<p><strong>Analysis:</strong> For ABC, the RL model generated significantly more long operations (70 vs 12) but fewer short operations (4 vs 9), consistent with its asymmetric threshold placement.</p>

<h2 id="profit--loss-calculation">Profit &amp; Loss Calculation</h2>

\[P\&amp;L_{\text{Short/Long}} = P_{t1_{\text{sell/buy}}}^{foreign} - P_{t1_{\text{buy/sell}}}^{local} - P_{t2_{\text{buy/sell}}}^{foreign} + P_{t2_{\text{sell/buy}}}^{local}\]

<p>where subscripts indicate timing (t1 = open, t2 = close) and superscripts indicate market (foreign/local).</p>

<h2 id="performance-metrics">Performance Metrics</h2>

<p>The following metrics were chosen to provide a comprehensive evaluation of both profitability and risk-adjusted performance for each model:</p>
<ul>
  <li><strong>Profit Metrics</strong>: Measure absolute and relative returns, as well as the consistency of winning trades.</li>
  <li><strong>Risk Metrics</strong>: Assess volatility, drawdowns, downside risk, and gain-loss ratios to capture the risk profile of each strategy.</li>
  <li><strong>Risk-Adjusted Metrics</strong>: Include Sharpe, Sortino, and Calmar ratios to evaluate returns relative to risk taken.</li>
</ul>

<p>This selection ensures a balanced comparison between the stochastic mean reversion (MR) and reinforcement learning approaches, highlighting both their strengths and limitations in practical pair trading scenarios.</p>

<table>
  <thead>
    <tr>
      <th>Metric Group</th>
      <th>Metric</th>
      <th>Asset</th>
      <th>Stochastic MR</th>
      <th>Reinforcement Learning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Profit Metrics</strong></td>
      <td>Profit Rate</td>
      <td>ABC</td>
      <td>939.38%</td>
      <td>855.06%</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>XYZ</td>
      <td>174.95%</td>
      <td>174.95%</td>
    </tr>
    <tr>
      <td> </td>
      <td>Win Rate</td>
      <td>ABC</td>
      <td>90.47%</td>
      <td>79.72%</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>XYZ</td>
      <td>51.88%</td>
      <td>51.88%</td>
    </tr>
    <tr>
      <td><strong>Risk Metrics</strong></td>
      <td>Volatility</td>
      <td>ABC</td>
      <td>57.8%</td>
      <td>459.31%</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>XYZ</td>
      <td>884.27%</td>
      <td>884.27%</td>
    </tr>
    <tr>
      <td> </td>
      <td>Max Drawdown</td>
      <td>ABC</td>
      <td>0</td>
      <td>-2.45</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>XYZ</td>
      <td>-2.54</td>
      <td>-2.54</td>
    </tr>
    <tr>
      <td> </td>
      <td>Downside Deviation</td>
      <td>ABC</td>
      <td>2.33</td>
      <td>2.42</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>XYZ</td>
      <td>0.177</td>
      <td>0.177</td>
    </tr>
    <tr>
      <td> </td>
      <td>Gain-Loss Ratio</td>
      <td>ABC</td>
      <td>5.12</td>
      <td>0.984</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>XYZ</td>
      <td>0.3484</td>
      <td>0.3484</td>
    </tr>
    <tr>
      <td><strong>Risk-Adjusted</strong></td>
      <td>Sharpe Ratio</td>
      <td>ABC</td>
      <td>2.73</td>
      <td>1.45</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>XYZ</td>
      <td>0.360</td>
      <td>0.360</td>
    </tr>
    <tr>
      <td> </td>
      <td>Sortino Ratio</td>
      <td>ABC</td>
      <td>3.43</td>
      <td>1.92</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>XYZ</td>
      <td>0.6808</td>
      <td>0.6808</td>
    </tr>
    <tr>
      <td> </td>
      <td>Calmar Ratio</td>
      <td>ABC</td>
      <td>∞</td>
      <td>-1.90</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>XYZ</td>
      <td>-0.0474</td>
      <td>-0.0474</td>
    </tr>
  </tbody>
</table>

<h3 id="key-findings">Key Findings</h3>

<p><strong>For ABC (stable mean reversion):</strong></p>
<ul>
  <li><strong>Stochastic model</strong> shows superior performance across most metrics:
    <ul>
      <li>Higher profit rate (939.38% vs 855.06%)</li>
      <li>Higher win rate (90.47% vs 79.72%)</li>
      <li>Much lower volatility (57.8% vs 459.31%)</li>
      <li>No drawdown vs -2.45</li>
      <li>Better risk-adjusted returns (Sharpe: 2.73 vs 1.45, Sortino: 3.43 vs 1.92)</li>
    </ul>
  </li>
  <li>RL model generates more trades but with higher volatility</li>
</ul>

<p><strong>For XYZ (structural breaks):</strong></p>
<ul>
  <li>Both models produce identical results</li>
  <li>Both models found very similar optimal thresholds</li>
  <li>Performance is inferior across all metrics compared to ABC</li>
  <li>This suggests the spread doesn’t follow stable mean reversion</li>
</ul>

<h1 id="conclusions">Conclusions</h1>

<h2 id="model-relationships">Model Relationships</h2>

<ol>
  <li>
    <p><strong>Monte Carlo Connection</strong>: Both RL and stochastic mean reversion models use Monte Carlo methods to find expected values along to profit and loss functions for the stochastic model and Q-values for RL.</p>
  </li>
  <li>
    <p><strong>Bellman Equation</strong>: Both approaches leverage the Bellman equation for optimization—explicitly in RL and through stochastic optimization in the mean reversion model. RL can be considered analogous to stochastic optimization using Monte Carlo methods.</p>
  </li>
</ol>

<h2 id="performance-analysis">Performance Analysis</h2>

<ol>
  <li>
    <p><strong>Spread Behavior Matters</strong>: Both models are effective for processes with stable mean reversion behavior (ABC) but show limited differentiation for spreads with structural breaks (XYZ).</p>
  </li>
  <li><strong>Asymmetric vs Symmetric Thresholds</strong>:
    <ul>
      <li>The stochastic model produces relatively symmetric levels around the mean</li>
      <li>The RL model shows asymmetric levels, closer to $\theta$ on the long side</li>
      <li>For ABC, the symmetric approach (stochastic) yielded better risk-adjusted returns</li>
    </ul>
  </li>
  <li><strong>Trading Horizon Impact</strong>: The threshold levels are influenced by the strategy’s cut-off horizon (30 days in this case).</li>
</ol>

<h2 id="practical-considerations">Practical Considerations</h2>

<p>While the RL model showed impressive absolute returns, several real-world frictions were not incorporated:</p>
<ul>
  <li>Transaction costs</li>
  <li>Bid-ask spreads</li>
  <li>Tax implications in each market</li>
  <li>Margin costs for short selling</li>
</ul>

<p>Within the academic exercise framework, <strong>the stochastic mean reversion model demonstrated superior performance</strong> for pair trading on assets with stable mean-reverting spreads, particularly when considering risk-adjusted metrics.</p>

<h2 id="advantages-of-each-approach">Advantages of Each Approach</h2>

<p><strong>Stochastic Mean Reversion Model:</strong></p>
<ul>
  <li>More predictable behavior</li>
  <li>Lower volatility</li>
  <li>Better risk-adjusted returns for stable spreads</li>
  <li>Simpler interpretation and implementation</li>
  <li>Symmetric thresholds around mean</li>
</ul>

<p><strong>Reinforcement Learning Model:</strong></p>
<ul>
  <li>Learns from both good and bad decisions through reward mechanisms</li>
  <li>Analogous to solving Bellman equations for optimization</li>
  <li>Can adapt to complex reward structures</li>
  <li>Potential for improvement with deep learning enhancements</li>
  <li>More flexible in asymmetric market conditions</li>
</ul>

<h2 id="future-research-directions">Future Research Directions</h2>

<ol>
  <li>
    <p><strong>Different Asset Pairs</strong>: Test models on assets that are not cross-listed, especially pairs with minor structural changes</p>
  </li>
  <li><strong>Deep Learning Integration</strong>: Enhance RL models with:
    <ul>
      <li>Deep Q-Networks (DQN)</li>
      <li>Deep deterministic policy gradients (DDPG)</li>
      <li>Convolutional layers for feature extraction</li>
      <li>Meta-learning approaches</li>
    </ul>
  </li>
  <li>
    <p><strong>Real-World Frictions</strong>: Incorporate transaction costs, slippage, and tax implications</p>
  </li>
  <li>
    <p><strong>Alternative Distributions</strong>: Explore other heavy-tailed distributions beyond GHYP for the diffusion factor</p>
  </li>
  <li><strong>Multi-Asset Strategies</strong>: Extend to portfolios of multiple pairs simultaneously</li>
</ol>

<h1 id="technical-implementation-notes">Technical Implementation Notes</h1>

<h2 id="monte-carlo-simulation-parameters">Monte Carlo Simulation Parameters</h2>

<p><strong>Stochastic Model:</strong></p>
<ul>
  <li>3 million random numbers generated</li>
  <li>Grid: 250 × 12,000 (250 days × 12,000 scenarios)</li>
  <li>GHYP parameters estimated via maximum likelihood</li>
</ul>

<p><strong>Reinforcement Learning:</strong></p>
<ul>
  <li>Q-value matrix dimensions: 504 × 3 (states × actions)</li>
  <li>Episode length: randomly selected 20-30 day windows</li>
  <li>Exploration: $\epsilon$-greedy strategy</li>
  <li>State space: discretized spread range</li>
</ul>

<h2 id="algorithm-pseudocode">Algorithm Pseudocode</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Initialize: episodes=5000, maxSteps=30, ε=ε*, α=α*, γ=γ*
Initialize: Q(nS, nA) ← 0

For episode_i = 1 to episodes:
    env ← reset_environment()
    a_t ← select_action(ε)
    step_t ← 0
    
    While not terminated:
        r_t ← reward(a_t)
        step_t ← step_t + 1
        s_t ← new_state
        a_t ← select_action(ε)
        Q(nS, nA) ← update_Q(a_t, a_{t-1}, s_t, s_{t-1})
        terminated ← evaluate(conditions)
</code></pre></div></div>

<p>Where:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">reset_environment()</code>: Randomly selects a time segment of length maxSteps</li>
  <li><code class="language-plaintext highlighter-rouge">select_action(ε)</code>: Implements ε-greedy policy</li>
  <li><code class="language-plaintext highlighter-rouge">update_Q()</code>: Applies Q-learning or SARSA update rule</li>
</ul>

<hr />

<h1 id="references">References</h1>

<p>This research draws on the intersection of stochastic calculus, optimization theory, and modern machine learning <a class="citation" href="#Baldi">(Baldi, 2017; Sutton &amp; Barto, 2020; Powell, 2022)</a>. The comparison reveals that while reinforcement learning shows promise, classical stochastic models remain highly effective for well-behaved mean-reverting processes <a class="citation" href="#Bertram09">(Bertram, 2009; Bertram, 2010; Leung &amp; Li, 2016)</a>, particularly when incorporating realistic risk management considerations.</p>

<p>The Ornstein-Uhlenbeck mean reversion model <a class="citation" href="#Ornstein">(Ornstein &amp; Uhlenbeck, 1930; Schwartz, 1997)</a> has been extensively studied for pair trading applications <a class="citation" href="#Goncu">(Goncu &amp; Akyildirim, 2016; Zeng &amp; Lee, 2014; Avellaneda &amp; Lee, 2010)</a>. The use of generalized hyperbolic distributions <a class="citation" href="#Konlack">(Konlack &amp; Wilcox, 2014; Weibel et al., 2022)</a> for modeling heavy-tailed distributions provides a more realistic representation of market dynamics compared to normal distributions <a class="citation" href="#Madan">(Madan et al., 1999; Carr &amp; Wu, 2004)</a>.</p>

<p>On the reinforcement learning side, Q-learning and SARSA algorithms <a class="citation" href="#Sutton">(Sutton &amp; Barto, 2020; Kaelbling et al., 1998)</a> have been applied to trading strategies <a class="citation" href="#Chakole">(Chakole et al., 2021; Carapuco et al., 2018; Wu et al., 2020)</a>. Recent studies have explored actor-critic methods <a class="citation" href="#Konda">(Konda &amp; Tsitsiklis, 2003)</a> and deep reinforcement learning approaches <a class="citation" href="#Plaat">(Plaat, 2022; Dong et al., 2020)</a> for financial applications <a class="citation" href="#Sun">(Sun et al., 2021; Carta et al., 2021; Kowalik et al., 2019)</a>.</p>

<p>The theoretical foundations draw from the Efficient Market Hypothesis <a class="citation" href="#Fama:1970">(Fama, 1970)</a> and its evolution into the Adaptive Market Hypothesis <a class="citation" href="#Lo">(Lo, 2004)</a>, acknowledging behavioral factors <a class="citation" href="#Shiller:2014">(Shiller, 2014)</a> and even chaotic dynamics <a class="citation" href="#Klioutchinov17">(Klioutchinov et al., 2017; Minsky, 1979)</a> in financial markets. The pair trading strategy itself is well-documented in quantitative finance literature <a class="citation" href="#Isichenko">(Isichenko, 2021; Chan, 2013; Vidyamurthi, 2004; Krauss, 2015)</a>, building on modern portfolio theory <a class="citation" href="#Elton">(Elton et al., 2014)</a>.</p>

<p>Parameter estimation techniques include maximum likelihood methods <a class="citation" href="#AitSahalia">(Ait-Sahala, 2002; Mejía, 2018; Dempster et al., 1977)</a> and specialized approaches for stochastic processes <a class="citation" href="#Haress">(Haress &amp; Hu, 2021; Minnis, 2012)</a>.</p>

<p>The key insight is not that one model is universally superior, but rather that <strong>model selection should be driven by the characteristics of the underlying spread dynamics</strong>. For stable mean-reverting spreads, stochastic models provide excellent risk-adjusted returns with lower volatility. For more complex or non-stationary spreads, enhanced RL approaches with deep learning may offer advantages—an avenue ripe for future exploration.</p>

<p><strong>Keywords</strong>: mean reversion, stochastic models, reinforcement learning, pair trading, statistical arbitrage, Ornstein-Uhlenbeck, Q-learning, SARSA, quantitative finance, algorithmic trading</p>

<hr />

<h2 id="bibliography">Bibliography</h2>

<ol class="bibliography"><li><span id="Lo">Lo, A. (2004). The adaptative market hypothesis: Market efficiency from an evolutionary perspective. <i>The Journal of Portfolio Management</i>.</span></li>
<li><span id="Isichenko">Isichenko, M. (2021). <i>Quantitative Portfolio Management: The art and science of statistical arbitrage</i>. Wiley Finance Series.</span></li>
<li><span id="Pole">Pole, A. (2007). <i>Statistical Arbitrage. Algorithmic trading insights and techniques</i>. Wiley Finance.</span></li>
<li><span id="Leung">Leung, T., &amp; Li, X. (2016). <i>Optimal mean reversion: Mathematical analysis and practical applications</i>. World Scientific.</span></li>
<li><span id="Wibel">Weibel, M., Breymann, W., &amp; Luthi, D. (2022). <i>gyph: A package on generalized hyperbolic distributions</i>. CRAN-repository.</span></li>
<li><span id="Dempster">Dempster, A. P., Laird, N. M., &amp; Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. <i>Journal of the Royal Statistical Society</i>, 1–22.</span></li>
<li><span id="Yuan">Yuan, Y. (2019). A novel multi-step Q-learning method to improve data efficiency for deep reinforcement learning. <i>Knowledge-Based Systems</i>, <i>175</i>, 107–117.</span></li>
<li><span id="Baldi">Baldi, P. (2017). <i>Stochastic calculus: An introduction through theory and excercises</i>. Springer.</span></li>
<li><span id="Sutton">Sutton, R., &amp; Barto, A. (2020). <i>Reinforcement learning: An introduction</i>. The MIT Press.</span></li>
<li><span id="Powell">Powell, W. (2022). <i>Reinforcement learning and stochastic optimization: An unified framework for sequential decitions</i>. Wiley.</span></li>
<li><span id="Bertram09">Bertram, W. (2009). Optimal trading Strategies for Ito diffusion processes. <i>Physica A: Statistical Mechanics and Its Applications</i>, <i>338</i>, 2865–2873.</span></li>
<li><span id="Bertram10">Bertram, W. (2010). Analytic solution for optimal statistical arbitrage trading. <i>Physica A: Statistical Mechanics and Its Applications</i>, <i>389</i>, 2234–2243.</span></li>
<li><span id="Ornstein">Ornstein, L., &amp; Uhlenbeck, G. (1930). On the theory of the brownian motion. <i>Physical Review Journal</i>, <i>36</i>(5), 823–841.</span></li>
<li><span id="Schwartz">Schwartz, E. (1997). Stochastic behavior of commodity prices: Implications for valuation and hedging. <i>Journal of Finance</i>, <i>52</i>(2), 923–973.</span></li>
<li><span id="Goncu">Goncu, A., &amp; Akyildirim, E. (2016). A stochastic model for commodity pairs trading. <i>Quantitative Finance</i>, <i>16</i>(12), 1843–1857.</span></li>
<li><span id="Zeng">Zeng, Z., &amp; Lee, C.-G. (2014). Pairs trading: optimal threshold and profitability. <i>Quantitative Finance</i>, <i>14</i>(11), 1881–1893.</span></li>
<li><span id="Avellaneda">Avellaneda, M., &amp; Lee, J.-H. (2010). Statistical arbitrage in the US equity markets. <i>Quantitative Finance</i>, <i>10</i>(7), 761–782.</span></li>
<li><span id="Konlack">Konlack, V., &amp; Wilcox, D. (2014). A comparison of generalized hyperbolic distribution models for equity returns. <i>Journal of Applied Mathematics</i>, 15.</span></li>
<li><span id="Madan">Madan, D., Carr, P., &amp; Chang, E. (1999). The Variance Gamma process and option pricing. <i>European Finance Review</i>, <i>2</i>(1), 79–105.</span></li>
<li><span id="Carr">Carr, P., &amp; Wu, L. (2004). Time-changed Levy processes and option pricing. <i>Journal of Financial Economics</i>, <i>71</i>(1), 113–141.</span></li>
<li><span id="Kaelbling">Kaelbling, L., Littman, M., &amp; Cassandra, A. (1998). Planning and acting in partially observable stochastic domains. <i>Artificial Intelligence</i>, <i>101</i>, 99–134.</span></li>
<li><span id="Chakole">Chakole, J., Kolhe, M., Mahapurush, G., Yadav, A., &amp; Kurhekar, M. (2021). A Q-learning agent for automated trading in equity stock markets. <i>Expert Systems with Applications</i>, <i>163</i>, 1–12.</span></li>
<li><span id="Carapuco">Carapuco, J., Neves, R., &amp; Horta, N. (2018). Reinforcement learning applied to forex trading. <i>Applied Soft Computing Journal</i>, <i>73</i>, 783–794.</span></li>
<li><span id="Wu">Wu, X., Chen, H., Wang, J., Troiano, L., Loia, V., &amp; Fujita, H. (2020). Adaptative stock trading strategies with deep reinforcement learning methods. <i>Information Science</i>, <i>538</i>, 142–158.</span></li>
<li><span id="Konda">Konda, V., &amp; Tsitsiklis, J. (2003). On actor-critic algorithms. <i>SIAM J. Control Optim</i>, <i>42</i>(4), 1143–1166.</span></li>
<li><span id="Plaat">Plaat, A. (2022). <i>Deep reinforcement learning</i>. Springer.</span></li>
<li><span id="Dong">Dong, H., Ding, Z., &amp; Zhang, S. (2020). <i>Deep reinforcement learning: Fundamentals, research and applications</i>. Springer.</span></li>
<li><span id="Sun">Sun, S., Wang, R., &amp; An, B. (2021). Reinforcement learning for quantitative trading. <i>ArXiv EPrint</i>.</span></li>
<li><span id="Carta">Carta, S., Corriga, A., Ferreira, A., &amp; Podda, A. (2021). A multi-layer and multi-ensemble stock trader using deep learning. <i>Applied Science</i>, <i>51</i>, 889–905.</span></li>
<li><span id="Kowalik:Thesis:2019">Kowalik, P., Kjellevold, A., &amp; Gropen, S. (2019). <i>A deep reinforcement learning approach for stock trading</i> [Master's thesis]. Norwegian University of Science and Technology.</span></li>
<li><span id="Fama:1970">Fama, E. (1970). Efficient capital markets: A review of theory and empirical work. <i>Journal of Finance</i>, <i>25</i>(2), 383–417.</span></li>
<li><span id="Shiller:2014">Shiller, R. (2014). Speculative Asset Prices. <i>The American Economic Review</i>, <i>104</i>(6), 1486–1517.</span></li>
<li><span id="Klioutchinov17">Klioutchinov, I., Sigova, M., &amp; Beizerov, N. (2017). Chaos theory in finance. <i>Procedia Computer Science</i>, <i>119</i>, 368–375.</span></li>
<li><span id="Minsky77">Minsky, H. (1979). The financial instability hypothesis: An interpretation of Keynes an alternative to ’standard’ theory. <i>Nebraska Journal of Economics, Business</i>, <i>16</i>(1), 5–18.</span></li>
<li><span id="Chan">Chan, E. (2013). <i>Algorithmic trading: winning strategies and their rationale</i>. Wiley Trading Series.</span></li>
<li><span id="Vidyamurthy">Vidyamurthi, G. (2004). <i>Pairs trading: Quantitative methods and analysis</i>. Wiley Finance.</span></li>
<li><span id="Krauss">Krauss, C. (2015). <i>Statistical arbitrage pairs trading strategies: Review and outlook</i> (No. 9; Number 9). Institut für Wirtschaftspolitik und Quantitative Wirtschaftsforschung, Nürnberg - Working Paper.</span></li>
<li><span id="Elton">Elton, E., Brown, S., Gruber, M., &amp; Goetzman, W. (2014). <i>Modern portfolio theory and investment analysis</i>. Wiley.</span></li>
<li><span id="AitSahalia">Ait-Sahala, Y. (2002). Maximum likelihood estimation of discretely sampled diffusions: A closed-form approximation approach. <i>Econometrica</i>, <i>70</i>(1), 223–262.</span></li>
<li><span id="Mejia">Mejía, C. (2018). Calibration of exponential Ornstein-Uhlenbeck process when spot prices are visible through the maximum log-likelihood method. Example with gold prices. <i>Spring Open Journal: Advances in Difference Equations</i>, 269.</span></li>
<li><span id="Haress">Haress, E., &amp; Hu, Y. (2021). Estimation of all parameters in the fractional Ornstein-Uhlenbeck model under discrete observations. <i>Statistical Inference for Stochastic Processes</i>, <i>24</i>, 327–351.</span></li>
<li><span id="Minnis">Minnis, M. (2012). Mean reverting Levy based processes. <i>SSRN Electronic Journal</i>. https://doi.org/10.2139/ssrn.2086485</span></li></ol>
</p>
        
			
		
			
        
			
		
			
        
			
		
			
        
			
		
			
        
			
		
			
        
			
		
			
        
			
		
			
        
			
		
			
        
			
		
			
        
			
		
			
        
			
		
	</div>
      </section>

    </div>

    <!-- Footer -->
<footer id="footer" class="minimalist-footer">
	<div class="inner">
		<!-- Social Icons with visible styling -->
		<div class="footer-social">
			<ul class="icons">
				
				<li><a href="https://twitter.com/_arleyquintero" class="icon alt fa-twitter" target="_blank" aria-label="Twitter"><span class="label">Twitter</span></a></li>
				
				
				
				
				
				
				
				
				<li><a href="https://github.com/craquinterogo/quantitative_finance" class="icon alt fa-github" target="_blank" aria-label="GitHub"><span class="label">GitHub</span></a></li>
				
				
				
				<li><a href="https://www.linkedin.com/in/craquinterogo" class="icon alt fa-linkedin" target="_blank" aria-label="LinkedIn"><span class="label">LinkedIn</span></a></li>
				
			</ul>
		</div>
		
		<!-- Email contact (minimalist) -->
		
		<div class="footer-email">
			<a href="mailto:craquinterogo@gmail.com" class="email-link">
				<i class="fa fa-envelope"></i> craquinterogo@gmail.com
			</a>
		</div>
		
		
		<!-- Copyright -->
		<div class="footer-copyright">
			<p>&copy; Cristian Quintero | Design: <a href="https://html5up.net" target="_blank">HTML5 UP</a></p>
		</div>
	</div>
</footer>

<style>
	/* Minimalist Footer Styles */
	.minimalist-footer {
		background: linear-gradient(135deg, #1a1a1a 0%, #242424 100%);
		padding: 3rem 0 2rem;
		text-align: center;
		border-top: 3px solid #d4af37;
	}
	
	.minimalist-footer .inner {
		max-width: 900px;
		margin: 0 auto;
		padding: 0 2rem;
	}
	
	/* Social Icons - Large, Visible, Animated */
	.footer-social {
		margin-bottom: 2rem;
	}
	
	.footer-social .icons {
		display: flex;
		justify-content: center;
		flex-wrap: wrap;
		gap: 1.5rem;
		list-style: none;
		padding: 0;
		margin: 0;
	}
	
	.footer-social .icons li {
		margin: 0;
	}
	
	.footer-social .icons a {
		display: flex;
		align-items: center;
		justify-content: center;
		width: 50px;
		height: 50px;
		background: rgba(212, 175, 55, 0.1);
		border: 2px solid rgba(212, 175, 55, 0.3);
		border-radius: 50%;
		color: #d4af37;
		font-size: 1.5rem;
		transition: all 0.3s ease;
		text-decoration: none;
	}
	
	.footer-social .icons a:hover {
		background: #d4af37;
		color: #1a1a1a;
		transform: translateY(-5px) scale(1.1);
		box-shadow: 0 10px 25px rgba(212, 175, 55, 0.4);
		border-color: #d4af37;
	}
	
	.footer-social .icons a .label {
		display: none;
	}
	
	/* Email Link */
	.footer-email {
		margin-bottom: 2rem;
	}
	
	.footer-email .email-link {
		display: inline-flex;
		align-items: center;
		gap: 0.5rem;
		padding: 0.75rem 1.5rem;
		background: rgba(212, 175, 55, 0.1);
		border: 1px solid rgba(212, 175, 55, 0.3);
		border-radius: 25px;
		color: #d4af37;
		text-decoration: none;
		font-size: 0.95rem;
		transition: all 0.3s ease;
	}
	
	.footer-email .email-link:hover {
		background: rgba(212, 175, 55, 0.2);
		border-color: #d4af37;
		transform: translateY(-2px);
		box-shadow: 0 5px 15px rgba(212, 175, 55, 0.3);
	}
	
	.footer-email .email-link i {
		font-size: 1.1rem;
	}
	
	/* Copyright */
	.footer-copyright {
		padding-top: 1.5rem;
		border-top: 1px solid rgba(212, 175, 55, 0.15);
	}
	
	.footer-copyright p {
		margin: 0;
		font-size: 0.85rem;
		color: rgba(232, 232, 232, 0.6);
		line-height: 1.6;
	}
	
	.footer-copyright a {
		color: #d4af37;
		text-decoration: none;
		transition: color 0.3s ease;
	}
	
	.footer-copyright a:hover {
		color: #c9a961;
	}
	
	/* Responsive */
	@media (max-width: 736px) {
		.minimalist-footer {
			padding: 2rem 0 1.5rem;
		}
		
		.footer-social .icons {
			gap: 1rem;
		}
		
		.footer-social .icons a {
			width: 45px;
			height: 45px;
			font-size: 1.3rem;
		}
		
		.footer-email .email-link {
			font-size: 0.85rem;
			padding: 0.6rem 1.2rem;
		}
		
		.footer-copyright p {
			font-size: 0.75rem;
		}
	}
</style>

</div>

<!-- Scripts -->
	<script src="/assets/js/jquery.min.js"></script>
	<script src="/assets/js/jquery.scrolly.min.js"></script>
	<script src="/assets/js/jquery.scrollex.min.js"></script>
	<script src="/assets/js/skel.min.js"></script>
	<script src="/assets/js/util.js"></script>
	<!--[if lte IE 8]><script src="/assets/js/ie/respond.min.js"></script><![endif]-->
	<script src="/assets/js/main.js"></script>


        <script>
      // Sidebar and collapsible sections functionality
      document.addEventListener('DOMContentLoaded', function() {
        const sidebar = document.getElementById('sidebar');
        const sidebarToggle = document.getElementById('sidebar-toggle');
        const sidebarOverlay = document.getElementById('sidebar-overlay');
        
        // Sidebar toggle for mobile
        if (sidebarToggle && sidebar && sidebarOverlay) {
          sidebarToggle.addEventListener('click', function() {
            sidebar.classList.toggle('active');
            sidebarOverlay.classList.toggle('active');
          });
          
          sidebarOverlay.addEventListener('click', function() {
            sidebar.classList.remove('active');
            sidebarOverlay.classList.remove('active');
          });
        }

        // Collapsible section headers
        const sectionHeaders = document.querySelectorAll('.sidebar-section-header');
        sectionHeaders.forEach(header => {
          header.addEventListener('click', function() {
            const targetId = this.getAttribute('data-target');
            const content = document.getElementById(targetId);
            const toggle = this.querySelector('.collapse-toggle i');
            
            if (content) {
              content.classList.toggle('expanded');
              content.classList.toggle('collapsed');
              
              // Rotate chevron icon
              if (content.classList.contains('expanded')) {
                toggle.classList.remove('fa-chevron-right');
                toggle.classList.add('fa-chevron-down');
              } else {
                toggle.classList.remove('fa-chevron-down');
                toggle.classList.add('fa-chevron-right');
              }
            }
          });
        });

        // --- Table of Contents Generation in Left Sidebar ---
        const tocNav = document.getElementById('toc-sidebar-nav');
        const mainContent = document.querySelector('#main .inner');
        const tocSection = document.querySelector('.toc-section');
        
        if (tocNav && mainContent) {
          const headings = mainContent.querySelectorAll('h1, h2, h3, h4');
          if (headings.length > 2) {
            let tocHtml = '<ul class="toc-list">';
            let numbers = [0, 0, 0, 0]; // For h1-h4
            let tocNumbers = [0, 0, 0, 0]; // Separate numbering for TOC
            
            // Start from the first heading (index 0)
            for (let idx = 0; idx < headings.length; idx++) {
              const heading = headings[idx];
              const level = parseInt(heading.tagName.substring(1)) - 1; // 0-based
              
              // Update numbering for content
              numbers[level]++;
              for (let i = level + 1; i < numbers.length; i++) numbers[i] = 0;
              
              // Build number string for content
              let numberStr = numbers.slice(0, level + 1).filter(n => n > 0).join('.');
              
              // Prepend number to heading text
              let headingText = heading.textContent.replace(/^\d+(\.\d+)*\.\s*/, '');
              heading.innerHTML = `<span class=\"section-number\">${numberStr}.</span> ` + headingText;
              
              if (!heading.id) {
                heading.id = headingText.trim().toLowerCase().replace(/[^a-z0-9]+/g, '-');
              }
              
              // Add all headings to TOC
              tocNumbers[level]++;
              for (let i = level + 1; i < tocNumbers.length; i++) tocNumbers[i] = 0;
              let tocNumberStr = tocNumbers.slice(0, level + 1).filter(n => n > 0).join('.');
              
              tocHtml += `<li class=\"toc-level-${level+1}\"><a href=\"#${heading.id}\"><span class='toc-number'>${tocNumberStr}.</span> ${headingText}</a></li>`;
            }
            
            tocHtml += '</ul>';
            tocNav.innerHTML = tocHtml;
            
            // Show TOC section
            if (tocSection) {
              tocSection.style.display = 'block';
            }
          } else {
            // Hide TOC section if not enough headings
            if (tocSection) {
              tocSection.style.display = 'none';
            }
          }
        }
      });
    </script>

  </body>

</html>
  </body>

</html>
